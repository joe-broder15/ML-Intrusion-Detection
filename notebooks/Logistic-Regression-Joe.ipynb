{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook i will explore the effectiveness of using logistic regression on the UNSW_NB15 intrusion detection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will preprocess the data by performing the log transformations and then encoding categorical features as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log transformation applied to numeric features (if present) in the dataset\n",
      "Log transformation applied to numeric features (if present) in the dataset\n",
      "One-hot encoding applied to categorical features in the dataset\n",
      "Updated categorical feature columns: ['proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_any', 'proto_argus', 'proto_aris', 'proto_arp', 'proto_ax.25', 'proto_bbn-rcc', 'proto_bna', 'proto_br-sat-mon', 'proto_cbt', 'proto_cftp', 'proto_chaos', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crtp', 'proto_crudp', 'proto_dcn', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_egp', 'proto_eigrp', 'proto_emcon', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_ggp', 'proto_gmtp', 'proto_gre', 'proto_hmp', 'proto_i-nlsp', 'proto_iatp', 'proto_ib', 'proto_icmp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_igmp', 'proto_igp', 'proto_il', 'proto_ip', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ipnip', 'proto_ippc', 'proto_ipv6', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_irtp', 'proto_isis', 'proto_iso-ip', 'proto_iso-tp4', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_leaf-1', 'proto_leaf-2', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mobile', 'proto_mtp', 'proto_mux', 'proto_narp', 'proto_netblt', 'proto_nsfnet-igp', 'proto_nvp', 'proto_ospf', 'proto_pgm', 'proto_pim', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_prm', 'proto_ptp', 'proto_pup', 'proto_pvp', 'proto_qnx', 'proto_rdp', 'proto_rsvp', 'proto_rtp', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sctp', 'proto_sdrp', 'proto_secure-vmtp', 'proto_sep', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_st2', 'proto_stp', 'proto_sun-nd', 'proto_swipe', 'proto_tcf', 'proto_tcp', 'proto_tlsp', 'proto_tp++', 'proto_trunk-1', 'proto_trunk-2', 'proto_ttp', 'proto_udp', 'proto_unas', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xnet', 'proto_xns-idp', 'proto_xtp', 'proto_zero', 'state_CON', 'state_ECO', 'state_FIN', 'state_INT', 'state_PAR', 'state_REQ', 'state_RST', 'state_URN', 'state_no', 'service_-', 'service_dhcp', 'service_dns', 'service_ftp', 'service_ftp-data', 'service_http', 'service_irc', 'service_pop3', 'service_radius', 'service_smtp', 'service_snmp', 'service_ssh', 'service_ssl', 'is_sm_ips_ports_0', 'is_sm_ips_ports_1', 'is_ftp_login_0', 'is_ftp_login_1', 'is_ftp_login_2', 'is_ftp_login_4']\n",
      "One-hot encoding applied to categorical features in the dataset\n",
      "Updated categorical feature columns: ['proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_any', 'proto_argus', 'proto_aris', 'proto_arp', 'proto_ax.25', 'proto_bbn-rcc', 'proto_bna', 'proto_br-sat-mon', 'proto_cbt', 'proto_cftp', 'proto_chaos', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crtp', 'proto_crudp', 'proto_dcn', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_egp', 'proto_eigrp', 'proto_emcon', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_ggp', 'proto_gmtp', 'proto_gre', 'proto_hmp', 'proto_i-nlsp', 'proto_iatp', 'proto_ib', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_igmp', 'proto_igp', 'proto_il', 'proto_ip', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ipnip', 'proto_ippc', 'proto_ipv6', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_irtp', 'proto_isis', 'proto_iso-ip', 'proto_iso-tp4', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_leaf-1', 'proto_leaf-2', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mobile', 'proto_mtp', 'proto_mux', 'proto_narp', 'proto_netblt', 'proto_nsfnet-igp', 'proto_nvp', 'proto_ospf', 'proto_pgm', 'proto_pim', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_prm', 'proto_ptp', 'proto_pup', 'proto_pvp', 'proto_qnx', 'proto_rdp', 'proto_rsvp', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sctp', 'proto_sdrp', 'proto_secure-vmtp', 'proto_sep', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_st2', 'proto_stp', 'proto_sun-nd', 'proto_swipe', 'proto_tcf', 'proto_tcp', 'proto_tlsp', 'proto_tp++', 'proto_trunk-1', 'proto_trunk-2', 'proto_ttp', 'proto_udp', 'proto_unas', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xnet', 'proto_xns-idp', 'proto_xtp', 'proto_zero', 'state_ACC', 'state_CLO', 'state_CON', 'state_FIN', 'state_INT', 'state_REQ', 'state_RST', 'service_-', 'service_dhcp', 'service_dns', 'service_ftp', 'service_ftp-data', 'service_http', 'service_irc', 'service_pop3', 'service_radius', 'service_smtp', 'service_snmp', 'service_ssh', 'service_ssl', 'is_sm_ips_ports_0', 'is_sm_ips_ports_1', 'is_ftp_login_0', 'is_ftp_login_1', 'is_ftp_login_2']\n",
      "Training data shape: (175341, 199)\n",
      "Testing data shape: (82332, 194)\n",
      "Any non-numeric columns remaining in Training data: False\n",
      "List of columns in the Training Data:\n",
      "['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'label', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_any', 'proto_argus', 'proto_aris', 'proto_arp', 'proto_ax.25', 'proto_bbn-rcc', 'proto_bna', 'proto_br-sat-mon', 'proto_cbt', 'proto_cftp', 'proto_chaos', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crtp', 'proto_crudp', 'proto_dcn', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_egp', 'proto_eigrp', 'proto_emcon', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_ggp', 'proto_gmtp', 'proto_gre', 'proto_hmp', 'proto_i-nlsp', 'proto_iatp', 'proto_ib', 'proto_icmp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_igmp', 'proto_igp', 'proto_il', 'proto_ip', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ipnip', 'proto_ippc', 'proto_ipv6', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_irtp', 'proto_isis', 'proto_iso-ip', 'proto_iso-tp4', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_leaf-1', 'proto_leaf-2', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mobile', 'proto_mtp', 'proto_mux', 'proto_narp', 'proto_netblt', 'proto_nsfnet-igp', 'proto_nvp', 'proto_ospf', 'proto_pgm', 'proto_pim', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_prm', 'proto_ptp', 'proto_pup', 'proto_pvp', 'proto_qnx', 'proto_rdp', 'proto_rsvp', 'proto_rtp', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sctp', 'proto_sdrp', 'proto_secure-vmtp', 'proto_sep', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_st2', 'proto_stp', 'proto_sun-nd', 'proto_swipe', 'proto_tcf', 'proto_tcp', 'proto_tlsp', 'proto_tp++', 'proto_trunk-1', 'proto_trunk-2', 'proto_ttp', 'proto_udp', 'proto_unas', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xnet', 'proto_xns-idp', 'proto_xtp', 'proto_zero', 'state_CON', 'state_ECO', 'state_FIN', 'state_INT', 'state_PAR', 'state_REQ', 'state_RST', 'state_URN', 'state_no', 'service_-', 'service_dhcp', 'service_dns', 'service_ftp', 'service_ftp-data', 'service_http', 'service_irc', 'service_pop3', 'service_radius', 'service_smtp', 'service_snmp', 'service_ssh', 'service_ssl', 'is_sm_ips_ports_0', 'is_sm_ips_ports_1', 'is_ftp_login_0', 'is_ftp_login_1', 'is_ftp_login_2', 'is_ftp_login_4']\n",
      "\n",
      "List of columns in the Testing Data:\n",
      "['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'label', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_any', 'proto_argus', 'proto_aris', 'proto_arp', 'proto_ax.25', 'proto_bbn-rcc', 'proto_bna', 'proto_br-sat-mon', 'proto_cbt', 'proto_cftp', 'proto_chaos', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crtp', 'proto_crudp', 'proto_dcn', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_egp', 'proto_eigrp', 'proto_emcon', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_ggp', 'proto_gmtp', 'proto_gre', 'proto_hmp', 'proto_i-nlsp', 'proto_iatp', 'proto_ib', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_igmp', 'proto_igp', 'proto_il', 'proto_ip', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ipnip', 'proto_ippc', 'proto_ipv6', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_irtp', 'proto_isis', 'proto_iso-ip', 'proto_iso-tp4', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_leaf-1', 'proto_leaf-2', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mobile', 'proto_mtp', 'proto_mux', 'proto_narp', 'proto_netblt', 'proto_nsfnet-igp', 'proto_nvp', 'proto_ospf', 'proto_pgm', 'proto_pim', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_prm', 'proto_ptp', 'proto_pup', 'proto_pvp', 'proto_qnx', 'proto_rdp', 'proto_rsvp', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sctp', 'proto_sdrp', 'proto_secure-vmtp', 'proto_sep', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_st2', 'proto_stp', 'proto_sun-nd', 'proto_swipe', 'proto_tcf', 'proto_tcp', 'proto_tlsp', 'proto_tp++', 'proto_trunk-1', 'proto_trunk-2', 'proto_ttp', 'proto_udp', 'proto_unas', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xnet', 'proto_xns-idp', 'proto_xtp', 'proto_zero', 'state_ACC', 'state_CLO', 'state_CON', 'state_FIN', 'state_INT', 'state_REQ', 'state_RST', 'service_-', 'service_dhcp', 'service_dns', 'service_ftp', 'service_ftp-data', 'service_http', 'service_irc', 'service_pop3', 'service_radius', 'service_smtp', 'service_snmp', 'service_ssh', 'service_ssl', 'is_sm_ips_ports_0', 'is_sm_ips_ports_1', 'is_ftp_login_0', 'is_ftp_login_1', 'is_ftp_login_2']\n"
     ]
    }
   ],
   "source": [
    "from typing import List  # Import type hints for better code clarity\n",
    "\n",
    "# Define lists of features for preprocessing:\n",
    "# - 'categorical_features' will be one-hot encoded.\n",
    "# - 'features_to_transform' will undergo a log transformation to reduce skewness.\n",
    "categorical_features: List[str] = [\"proto\", \"state\", \"service\", \"is_sm_ips_ports\", \"is_ftp_login\"]\n",
    "features_to_transform: List[str] = [\n",
    "    'sbytes', 'dbytes', 'sttl', 'dttl', 'sload', 'dload', 'spkts', 'dpkts', \n",
    "    'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'sjit', 'djit'\n",
    "]\n",
    "\n",
    "# Load the training and testing datasets from CSV files.\n",
    "train_data: pd.DataFrame = pd.read_csv('../data/UNSW_NB15/UNSW_NB15_training-set.csv')\n",
    "test_data: pd.DataFrame = pd.read_csv('../data/UNSW_NB15/UNSW_NB15_testing-set.csv')\n",
    "\n",
    "# Clean both datasets by removing columns that are not needed for modeling.\n",
    "columns_to_drop = ['attack_cat', 'id']\n",
    "for df in [train_data, test_data]:\n",
    "    for col in columns_to_drop:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "def process_numeric_features(df: pd.DataFrame, features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies a natural logarithm transformation (ln(x+1)) to each specified numeric feature \n",
    "    in the provided dataframe. This helps to normalize the distribution of features and \n",
    "    mitigate the effect of extreme values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # apply the log transformation to the features that were determined in EDA\n",
    "    for feature in features:\n",
    "        if feature in df.columns:\n",
    "            df[feature] = np.log1p(df[feature])\n",
    "    print(\"Log transformation applied to numeric features (if present) in the dataset\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_categorical_features(df: pd.DataFrame, cat_features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One-hot encodes specified categorical features in the provided dataframe.\n",
    "    Processes each feature independently. If a feature is missing, a warning is printed.\n",
    "    Returns:\n",
    "        df: Updated dataframe with one-hot encoded categorical features.\n",
    "        updated_dummy_cols: List of the names of all the new categorical features.\n",
    "    \"\"\"\n",
    "    for feature in cat_features:\n",
    "        if feature in df.columns:\n",
    "            dummies = pd.get_dummies(df[feature].astype(str), prefix=feature)\n",
    "            df = df.drop(columns=[feature])\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "        else:\n",
    "            print(f\"Warning: '{feature}' not found in the dataframe; skipping one-hot encoding for this feature.\")\n",
    "    print(\"One-hot encoding applied to categorical features in the dataset\")\n",
    "    updated_dummy_cols = [col for col in df.columns if any(col.startswith(f\"{feature}_\") for feature in cat_features)]\n",
    "    print(\"Updated categorical feature columns:\", updated_dummy_cols)\n",
    "    return df, updated_dummy_cols\n",
    "\n",
    "# Process numeric features on each dataset independently.\n",
    "train_data = process_numeric_features(train_data, features_to_transform)\n",
    "test_data = process_numeric_features(test_data, features_to_transform)\n",
    "\n",
    "# Process categorical features on each dataset independently.\n",
    "train_data, train_categorical_features = process_categorical_features(train_data, categorical_features)\n",
    "test_data, test_categorical_features = process_categorical_features(test_data, categorical_features)\n",
    "\n",
    "# Calculate the union of the new categorical feature sets from training and testing datasets.\n",
    "# (The previous code was calculating the intersection.)\n",
    "categorical_features = [i for i in train_categorical_features if i in test_categorical_features]\n",
    "\n",
    "# Output the shapes of the processed datasets and confirm that all features are numeric.\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "print(f\"Any non-numeric columns remaining in Training data: {any(not pd.api.types.is_numeric_dtype(train_data[col]) for col in train_data.columns)}\")\n",
    "print(\"List of columns in the Training Data:\")\n",
    "print(list(train_data.columns))\n",
    "print(\"\\nList of columns in the Testing Data:\")\n",
    "print(list(test_data.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function that will do k-fold cross validation and train our model. this willbe used throughout the remainder of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_logistic_regression_cv(df, feature_columns, label_column='label', n_splits=5, random_state=42,\n",
    "                                   penalty='l2', C=1.0, solver='saga', max_iter=5000, tol=1e-4, n_jobs=-1, verbose=False):\n",
    "    # This function performs k-fold cross-validation for a logistic regression model.\n",
    "    # The model uses the hyperparameters provided as additional parameters. The defaults are set as:\n",
    "    #   penalty: 'l2'\n",
    "    #   C: 1.0\n",
    "    #   solver: 'saga'\n",
    "    #   max_iter: 5000\n",
    "    #   tol: 1e-4\n",
    "    #   n_jobs: -1\n",
    "    #   verbose: False\n",
    "    \n",
    "    # Import necessary libraries for model training and evaluation.\n",
    "    from sklearn.linear_model import LogisticRegression  # For building the logistic regression model\n",
    "    from sklearn.model_selection import KFold            # For splitting data into folds for cross-validation\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix  # To compute performance metrics\n",
    "    import numpy as np                                     # For numerical computations (mean, std, etc.)\n",
    "    \n",
    "    # Convert the provided dataframe columns into numpy arrays for efficient computation.\n",
    "    X = df[feature_columns].values  # Feature matrix constructed from specified columns\n",
    "    y = df[label_column].values       # Target vector extracted from the label column\n",
    "    \n",
    "    # Set up K-Fold cross-validation with shuffling for randomness.\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Initialize lists to store metrics for each fold.\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "    tpr_list, fpr_list, tnr_list, fnr_list = [], [], [], []\n",
    "    \n",
    "    # Iterate over each train-test split generated by KFold.\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the dataset into training and test sets for the current fold.\n",
    "        X_train, X_test = X[train_index], X[test_index]  # Extract training and testing features.\n",
    "        y_train, y_test = y[train_index], y[test_index]    # Extract corresponding target labels.\n",
    "        \n",
    "        # Initialize the Logistic Regression model with provided hyperparameters.\n",
    "        model = LogisticRegression(\n",
    "            penalty=penalty,              # Regularization method.\n",
    "            C=C,                          # Inverse of regularization strength.\n",
    "            solver=solver,                # Algorithm to use in the optimization problem.\n",
    "            max_iter=max_iter,            # Maximum number of iterations to ensure convergence.\n",
    "            tol=tol,                      # Tolerance for convergence.\n",
    "            random_state=random_state,    # Set random state for reproducibility.\n",
    "            n_jobs=n_jobs,                # Utilize the specified number of CPU cores.\n",
    "            verbose=verbose               # Verbose output mode.\n",
    "        )\n",
    "        \n",
    "        # Fit the model on the training data.\n",
    "        model.fit(X_train, y_train)\n",
    "        # Use the trained model to predict target labels on the test set.\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Compute performance metrics for the current fold and append them to the lists.\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))               # Overall accuracy of predictions.\n",
    "        precisions.append(precision_score(y_test, y_pred, zero_division=0))  # Precision; handles division by zero.\n",
    "        recalls.append(recall_score(y_test, y_pred, zero_division=0))         # Recall; handles division by zero.\n",
    "        f1_scores.append(f1_score(y_test, y_pred, zero_division=0))           # F1 score; harmonic mean of precision and recall.\n",
    "        \n",
    "        # Generate confusion matrix and unpack into true negatives, false positives, false negatives, and true positives.\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        # Calculate and store the True Positive Rate (Sensitivity) for the fold.\n",
    "        tpr_list.append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        # Calculate and store the False Positive Rate for the fold.\n",
    "        fpr_list.append(fp / (fp + tn) if (fp + tn) > 0 else 0)\n",
    "        # Calculate and store the True Negative Rate (Specificity) for the fold.\n",
    "        tnr_list.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "        # Calculate and store the False Negative Rate for the fold.\n",
    "        fnr_list.append(fn / (fn + tp) if (fn + tp) > 0 else 0)\n",
    "    \n",
    "    # Compile the computed metrics into a dictionary by calculating the mean and standard deviation across folds.\n",
    "    results = {\n",
    "        'accuracy': {'mean': np.mean(accuracies), 'std': np.std(accuracies)},\n",
    "        'precision': {'mean': np.mean(precisions), 'std': np.std(precisions)},\n",
    "        'recall': {'mean': np.mean(recalls), 'std': np.std(recalls)},\n",
    "        'f1': {'mean': np.mean(f1_scores), 'std': np.std(f1_scores)},\n",
    "        'true_positive_rate': {'mean': np.mean(tpr_list), 'std': np.std(tpr_list)},\n",
    "        'false_positive_rate': {'mean': np.mean(fpr_list), 'std': np.std(fpr_list)},\n",
    "        'true_negative_rate': {'mean': np.mean(tnr_list), 'std': np.std(tnr_list)},\n",
    "        'false_negative_rate': {'mean': np.mean(fnr_list), 'std': np.std(fnr_list)}\n",
    "    }\n",
    "    # Return the aggregated cross-validation results.\n",
    "    return results\n",
    "\n",
    "def pretty_print_results(results):\n",
    "    # Iterate through each metric in the results dictionary.\n",
    "    for metric, values in results.items():\n",
    "        # Format and print each metric's name, mean, and standard deviation.\n",
    "        print(f\"{metric.replace('_', ' ').capitalize()}: {values['mean']:.4f} (Â±{values['std']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will do some experimentation on sets of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 percentile features:\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate']\n",
      "\n",
      "Top 40 percentile features:\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate', 'swin', 'sload', 'dwin', 'stcpb', 'dtcpb', 'ct_src_dport_ltm', 'ct_dst_src_ltm', 'sbytes', 'dttl']\n",
      "\n",
      "Top 60 percentile features:\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate', 'swin', 'sload', 'dwin', 'stcpb', 'dtcpb', 'ct_src_dport_ltm', 'ct_dst_src_ltm', 'sbytes', 'dttl', 'ct_src_ltm', 'ct_dst_ltm', 'ct_srv_src', 'ct_srv_dst', 'sinpkt', 'djit', 'sjit', 'ackdat', 'dloss']\n",
      "\n",
      "Top 80 percentile features:\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate', 'swin', 'sload', 'dwin', 'stcpb', 'dtcpb', 'ct_src_dport_ltm', 'ct_dst_src_ltm', 'sbytes', 'dttl', 'ct_src_ltm', 'ct_dst_ltm', 'ct_srv_src', 'ct_srv_dst', 'sinpkt', 'djit', 'sjit', 'ackdat', 'dloss', 'tcprtt', 'synack', 'dur', 'dinpkt', 'response_body_len', 'ct_flw_http_mthd', 'proto_icmp', 'state_ECO', 'ct_ftp_cmd']\n",
      "\n",
      "All correlated features (in sorted order):\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate', 'swin', 'sload', 'dwin', 'stcpb', 'dtcpb', 'ct_src_dport_ltm', 'ct_dst_src_ltm', 'sbytes', 'dttl', 'ct_src_ltm', 'ct_dst_ltm', 'ct_srv_src', 'ct_srv_dst', 'sinpkt', 'djit', 'sjit', 'ackdat', 'dloss', 'tcprtt', 'synack', 'dur', 'dinpkt', 'response_body_len', 'ct_flw_http_mthd', 'proto_icmp', 'state_ECO', 'ct_ftp_cmd', 'trans_depth', 'smean', 'is_ftp_login_4', 'proto_rtp', 'state_URN', 'state_no', 'state_PAR', 'sloss']\n"
     ]
    }
   ],
   "source": [
    "# Detailed Numeric Feature Selection Process\n",
    "# ------------------------------------------------------------\n",
    "# This section identifies and ranks numeric features based on their absolute Pearson correlation\n",
    "# with the target label. We exclude both the 'label' column and any categorical features.\n",
    "#\n",
    "# The process involves:\n",
    "# 1. Extracting numeric features from the training dataset.\n",
    "# 2. Computing the absolute correlation of each feature with the target variable.\n",
    "# 3. Sorting features in descending order based on their correlation.\n",
    "# 4. Creating feature subsets corresponding to the top 20%, 40%, 60%, and 80% of features,\n",
    "#    in addition to a full sorted list of all features.\n",
    "#\n",
    "# These feature subsets can help in selecting the most impactful variables for model training.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Step 1: Extract numeric features by excluding 'label' and categorical features.\n",
    "numeric_features = [col for col in train_data.columns if col != 'label' and col not in categorical_features]\n",
    "\n",
    "# Safety check: Remove 'label' if it is inadvertently included.\n",
    "if 'label' in numeric_features:\n",
    "    numeric_features.remove('label')\n",
    "\n",
    "# Step 2: Compute the absolute Pearson correlation between each numeric feature and the target label.\n",
    "correlations = []\n",
    "for feature in numeric_features:\n",
    "    try:\n",
    "        corr_value = abs(train_data[feature].corr(train_data['label']))\n",
    "        correlations.append((feature, corr_value))\n",
    "    except KeyError:\n",
    "        print(f\"Error: Unable to calculate correlation for feature '{feature}'.\")\n",
    "\n",
    "# Step 3: Sort the features by their correlation strength in descending order.\n",
    "sorted_correlations = sorted(correlations, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Step 4: Determine indices for the top percentiles.\n",
    "n_features = len(sorted_correlations)\n",
    "index_20 = int(np.ceil(n_features * 0.20))\n",
    "index_40 = int(np.ceil(n_features * 0.40))\n",
    "index_60 = int(np.ceil(n_features * 0.60))\n",
    "index_80 = int(np.ceil(n_features * 0.80))\n",
    "\n",
    "# Create lists of features for each specified percentile.\n",
    "top_20_numeric_features = [feature for feature, _ in sorted_correlations[:index_20]]\n",
    "top_40_numeric_features = [feature for feature, _ in sorted_correlations[:index_40]]\n",
    "top_60_numeric_features = [feature for feature, _ in sorted_correlations[:index_60]]\n",
    "top_80_numeric_features = [feature for feature, _ in sorted_correlations[:index_80]]\n",
    "all_correlated_features = [feature for feature, _ in sorted_correlations]\n",
    "\n",
    "# Display the feature groups.\n",
    "print(\"\\nTop 20 percentile features:\")\n",
    "print(top_20_numeric_features)\n",
    "\n",
    "print(\"\\nTop 40 percentile features:\")\n",
    "print(top_40_numeric_features)\n",
    "\n",
    "print(\"\\nTop 60 percentile features:\")\n",
    "print(top_60_numeric_features)\n",
    "\n",
    "print(\"\\nTop 80 percentile features:\")\n",
    "print(top_80_numeric_features)\n",
    "\n",
    "print(\"\\nAll correlated features (in sorted order):\")\n",
    "print(all_correlated_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 percentile categorical features:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n']\n",
      "\n",
      "Top 40 percentile categorical features:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_aris', 'proto_ax.25', 'proto_bna', 'proto_br-sat-mon', 'proto_cftp', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crudp', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_eigrp', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_gmtp', 'proto_i-nlsp', 'proto_iatp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_il', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ippc']\n",
      "\n",
      "Top 60 percentile categorical features:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_aris', 'proto_ax.25', 'proto_bna', 'proto_br-sat-mon', 'proto_cftp', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crudp', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_eigrp', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_gmtp', 'proto_i-nlsp', 'proto_iatp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_il', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ippc', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_isis', 'proto_iso-ip', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mtp', 'proto_narp', 'proto_nsfnet-igp', 'proto_pgm', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_ptp', 'proto_pvp', 'proto_qnx', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sdrp', 'proto_secure-vmtp']\n",
      "\n",
      "Top 80 percentile categorical features:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_aris', 'proto_ax.25', 'proto_bna', 'proto_br-sat-mon', 'proto_cftp', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crudp', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_eigrp', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_gmtp', 'proto_i-nlsp', 'proto_iatp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_il', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ippc', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_isis', 'proto_iso-ip', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mtp', 'proto_narp', 'proto_nsfnet-igp', 'proto_pgm', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_ptp', 'proto_pvp', 'proto_qnx', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sdrp', 'proto_secure-vmtp', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_stp', 'proto_tcf', 'proto_tlsp', 'proto_tp++', 'proto_ttp', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xtp', 'proto_zero', 'proto_cbt', 'proto_chaos', 'proto_crtp', 'proto_dcn', 'proto_emcon', 'proto_ggp', 'proto_igp', 'proto_ip', 'proto_ipnip']\n",
      "\n",
      "All categorical features sorted by Chi-squared score:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_aris', 'proto_ax.25', 'proto_bna', 'proto_br-sat-mon', 'proto_cftp', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crudp', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_eigrp', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_gmtp', 'proto_i-nlsp', 'proto_iatp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_il', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ippc', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_isis', 'proto_iso-ip', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mtp', 'proto_narp', 'proto_nsfnet-igp', 'proto_pgm', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_ptp', 'proto_pvp', 'proto_qnx', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sdrp', 'proto_secure-vmtp', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_stp', 'proto_tcf', 'proto_tlsp', 'proto_tp++', 'proto_ttp', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xtp', 'proto_zero', 'proto_cbt', 'proto_chaos', 'proto_crtp', 'proto_dcn', 'proto_emcon', 'proto_ggp', 'proto_igp', 'proto_ip', 'proto_ipnip', 'proto_irtp', 'proto_iso-tp4', 'proto_leaf-1', 'proto_leaf-2', 'proto_mux', 'proto_nvp', 'proto_prm', 'proto_pup', 'proto_st2', 'proto_trunk-1', 'proto_trunk-2', 'proto_xnet', 'proto_xns-idp', 'proto_argus', 'proto_bbn-rcc', 'proto_egp', 'proto_hmp', 'proto_netblt', 'proto_rdp', 'service_dhcp', 'proto_igmp', 'service_snmp', 'is_ftp_login_1', 'service_ssl', 'service_ftp', 'service_irc', 'service_radius', 'service_smtp', 'is_ftp_login_0', 'is_ftp_login_2']\n"
     ]
    }
   ],
   "source": [
    "# Since the categorical features have already been one-hot encoded, we can use them directly.\n",
    "onehot_cat_features = train_data[categorical_features]\n",
    "\n",
    "# Step 2: Compute Chi-squared scores for each one-hot encoded categorical feature using the target label.\n",
    "from sklearn.feature_selection import chi2\n",
    "chi2_scores, p_values = chi2(onehot_cat_features, train_data['label'])\n",
    "\n",
    "# Step 3: Pair each categorical feature with its Chi-squared score and sort in descending order.\n",
    "cat_chi2_scores = list(zip(categorical_features, chi2_scores))\n",
    "sorted_cat_chi2 = sorted(cat_chi2_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 4: Determine indices corresponding to the top percentiles of categorical features.\n",
    "n_cat_features = len(sorted_cat_chi2)\n",
    "index_20_cat = int(np.ceil(n_cat_features * 0.20))\n",
    "index_40_cat = int(np.ceil(n_cat_features * 0.40))\n",
    "index_60_cat = int(np.ceil(n_cat_features * 0.60))\n",
    "index_80_cat = int(np.ceil(n_cat_features * 0.80))\n",
    "\n",
    "# Step 5: Create lists of categorical features for each specified percentile.\n",
    "top_20_cat_features = [feature for feature, _ in sorted_cat_chi2[:index_20_cat]]\n",
    "top_40_cat_features = [feature for feature, _ in sorted_cat_chi2[:index_40_cat]]\n",
    "top_60_cat_features = [feature for feature, _ in sorted_cat_chi2[:index_60_cat]]\n",
    "top_80_cat_features = [feature for feature, _ in sorted_cat_chi2[:index_80_cat]]\n",
    "all_chi2_cat_features = [feature for feature, _ in sorted_cat_chi2]\n",
    "\n",
    "# Step 6: Display the ranked categorical feature groups.\n",
    "print(\"\\nTop 20 percentile categorical features:\")\n",
    "print(top_20_cat_features)\n",
    "\n",
    "print(\"\\nTop 40 percentile categorical features:\")\n",
    "print(top_40_cat_features)\n",
    "\n",
    "print(\"\\nTop 60 percentile categorical features:\")\n",
    "print(top_60_cat_features)\n",
    "\n",
    "print(\"\\nTop 80 percentile categorical features:\")\n",
    "print(top_80_cat_features)\n",
    "\n",
    "print(\"\\nAll categorical features sorted by Chi-squared score:\")\n",
    "print(all_chi2_cat_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we present a hill climbing function that we will use to select different sets of features based on what we have calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def hill_climbing_feature_selection(df, \n",
    "                                    all_features, \n",
    "                                    tuning_size=0.2, \n",
    "                                    folds=5, \n",
    "                                    hill_max_iter=100,\n",
    "                                    min_features=10, \n",
    "                                    max_features=20,\n",
    "                                    epsilon=0.05,\n",
    "                                    label_column='label', \n",
    "                                    random_state=42, \n",
    "                                    penalty='l2', \n",
    "                                    C=1.0, \n",
    "                                    solver='saga', \n",
    "                                    max_iter_lr=10000, \n",
    "                                    tol=1e-4, \n",
    "                                    n_jobs=-1, \n",
    "                                    verbose=False):\n",
    "    \"\"\"\n",
    "    Select features via hill climbing to maximize CV F1 score using logistic regression.\n",
    "\n",
    "    Strategy:\n",
    "      - Sample a tuning set.\n",
    "      - Maintain two lists: available_features and used_features.\n",
    "      - At each iteration, randomly choose a swap, remove, or add move.\n",
    "      - Accept the move if F1 improves or with probability epsilon.\n",
    "\n",
    "    Parameters:\n",
    "      df: DataFrame for tuning.\n",
    "      all_features: List of candidate features.\n",
    "      tuning_size: Fraction of data for tuning.\n",
    "      folds: Number of CV folds.\n",
    "      hill_max_iter: Maximum iterations.\n",
    "      min_features: Minimum features to use.\n",
    "      max_features: Maximum features allowed.\n",
    "      epsilon: Chance to accept a non-improving move.\n",
    "      label_column: Name of the target column.\n",
    "      random_state: Seed for reproducibility.\n",
    "      penalty, C, solver, max_iter_lr, tol, n_jobs, verbose: Logistic regression settings.\n",
    "    \n",
    "    Returns:\n",
    "      best_features: Selected feature subset.\n",
    "      best_score: Achieved CV F1 score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check tuning_size.\n",
    "    if not (0 < tuning_size < 1):\n",
    "        raise ValueError(\"tuning_size must be a float between 0 and 1.\")\n",
    "    \n",
    "    # Sample tuning set.\n",
    "    tuning_df = df.sample(frac=tuning_size, random_state=random_state)\n",
    "    \n",
    "    # Evaluate a feature subset via CV.\n",
    "    def evaluate_features(feature_subset):\n",
    "        if not feature_subset:\n",
    "            return {'f1': 0.0, 'accuracy': 0.0}\n",
    "        results = perform_logistic_regression_cv(\n",
    "            tuning_df,\n",
    "            feature_subset,\n",
    "            label_column=label_column,\n",
    "            n_splits=folds,\n",
    "            random_state=random_state,\n",
    "            penalty=penalty,\n",
    "            C=C,\n",
    "            solver=solver,\n",
    "            max_iter=max_iter_lr,\n",
    "            tol=tol,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        return {'f1': results['f1']['mean'], 'accuracy': results['accuracy']['mean']}\n",
    "    \n",
    "    # Initialize feature lists.\n",
    "    available_features = all_features.copy()\n",
    "    used_features = list(np.random.choice(available_features, size=min_features, replace=False))\n",
    "    for feature in used_features:\n",
    "        available_features.remove(feature)\n",
    "    \n",
    "    # Evaluate initial features.\n",
    "    current_metrics = evaluate_features(used_features)\n",
    "    current_score = current_metrics['f1']\n",
    "    current_accuracy = current_metrics['accuracy']\n",
    "    best_features = used_features.copy()\n",
    "    best_score = current_score\n",
    "    best_accuracy = current_accuracy\n",
    "\n",
    "    for iteration in tqdm(range(hill_max_iter), desc=\"Hill Climbing Iterations\"):\n",
    "        # Randomly choose a move.\n",
    "        move_type = np.random.choice(['swap', 'remove', 'add'])\n",
    "        \n",
    "        # Copy current feature lists.\n",
    "        candidate_used = used_features.copy()\n",
    "        candidate_available = available_features.copy()\n",
    "        \n",
    "        if move_type == 'swap':\n",
    "            # Swap a feature if available.\n",
    "            if candidate_available:\n",
    "                feature_out = np.random.choice(candidate_used)\n",
    "                feature_in = np.random.choice(candidate_available)\n",
    "                candidate_used.remove(feature_out)\n",
    "                candidate_available.append(feature_out)\n",
    "                candidate_available.remove(feature_in)\n",
    "                candidate_used.append(feature_in)\n",
    "                \n",
    "        elif move_type == 'remove':\n",
    "            # Remove a feature if used count exceeds min_features.\n",
    "            if len(candidate_used) > min_features:\n",
    "                feature = np.random.choice(candidate_used)\n",
    "                candidate_used.remove(feature)\n",
    "                candidate_available.append(feature)\n",
    "            else:\n",
    "                continue  # Skip removal.\n",
    "                \n",
    "        elif move_type == 'add':\n",
    "            # Add a feature if under max_features.\n",
    "            if len(candidate_used) < max_features and candidate_available:\n",
    "                feature = np.random.choice(candidate_available)\n",
    "                candidate_used.append(feature)\n",
    "                candidate_available.remove(feature)\n",
    "            else:\n",
    "                continue  # Skip addition.\n",
    "        \n",
    "        # Evaluate candidate feature subset.\n",
    "        candidate_metrics = evaluate_features(candidate_used)\n",
    "        candidate_score = candidate_metrics['f1']\n",
    "        candidate_accuracy = candidate_metrics['accuracy']\n",
    "        \n",
    "        # Epsilon-greedy: Accept move if score improves or with epsilon probability\n",
    "        if candidate_score > current_score or np.random.random() < epsilon:\n",
    "            used_features = candidate_used.copy()\n",
    "            available_features = candidate_available.copy()\n",
    "            current_score = candidate_score\n",
    "            current_accuracy = candidate_accuracy\n",
    "            \n",
    "            # Update best if the current solution is better\n",
    "            if candidate_score > best_score:\n",
    "                best_features = candidate_used.copy()\n",
    "                best_score = candidate_score\n",
    "                best_accuracy = candidate_accuracy\n",
    "\n",
    "            print(f\"Iteration {iteration+1}: F1 = {current_score:.4f}, Accuracy = {current_accuracy:.4f}, Features = {used_features}\")\n",
    "            if candidate_score <= current_score:\n",
    "                print(f\"  (Accepted suboptimal move with epsilon={epsilon})\")\n",
    "    \n",
    "    print(f\"Hill climbing ended after {hill_max_iter} iterations.\")\n",
    "    print(f\"Best F1 = {best_score:.4f}, Best Accuracy = {best_accuracy:.4f}\")\n",
    "    return best_features, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730f4f2b5baf40c48fb2d922ac483d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hill Climbing Iterations:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4: F1 = 0.8361, Accuracy = 0.7459, Features = ['service_dns', 'proto_unas', 'is_sm_ips_ports_0', 'proto_ipv6', 'dmean', 'service_pop3']\n",
      "  (Accepted suboptimal move with epsilon=0.05)\n",
      "Iteration 6: F1 = 0.8248, Accuracy = 0.7262, Features = ['service_dns', 'proto_unas', 'is_sm_ips_ports_0', 'dmean', 'service_pop3', 'swin']\n",
      "  (Accepted suboptimal move with epsilon=0.05)\n",
      "Iteration 7: F1 = 0.8360, Accuracy = 0.7458, Features = ['service_dns', 'proto_unas', 'is_sm_ips_ports_0', 'dmean', 'service_pop3', 'proto_ospf']\n",
      "  (Accepted suboptimal move with epsilon=0.05)\n",
      "Iteration 9: F1 = 0.8365, Accuracy = 0.7465, Features = ['service_dns', 'proto_unas', 'is_sm_ips_ports_0', 'dmean', 'service_pop3', 'proto_ospf', 'proto_udp']\n",
      "  (Accepted suboptimal move with epsilon=0.05)\n"
     ]
    }
   ],
   "source": [
    "# Hill climbing feature selection\n",
    "\n",
    "best_features, best_score = hill_climbing_feature_selection(train_data, top_40_numeric_features + top_20_cat_features, folds=3, hill_max_iter=100, min_features=5, max_features=10)\n",
    "print(f\"Best F1 Score = {best_score:.4f}\")\n",
    "print(f\"Best Features = {best_features}\")\n",
    "\n",
    "# run cv on the full dataset with the best features\n",
    "results = perform_logistic_regression_cv(train_data, best_features, label_column='label', n_splits=5)\n",
    "pretty_print_results(results)\n",
    "\n",
    "# best_features = ['proto_3pc', 'proto_sctp', 'sttl', 'proto_tcp', 'state_FIN', 'state_REQ', 'spkts', 'ct_state_ttl', 'state_CON', 'proto_sun-nd', 'proto_arp', 'dpkts']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've selected good features, we will do hyperparameter tuning below\n",
    "```\n",
    "Best Features = ['proto_3pc', 'proto_sctp', 'sttl', 'proto_tcp', 'state_FIN', 'state_REQ', 'spkts', 'ct_state_ttl', 'state_CON', 'proto_sun-nd', 'proto_arp', 'dpkts']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning with best features:  ['proto_3pc', 'proto_sctp', 'sttl', 'proto_tcp', 'state_FIN', 'state_REQ', 'spkts', 'ct_state_ttl', 'state_CON', 'proto_sun-nd', 'proto_arp', 'dpkts']\n",
      "Starting grid search with 324 parameter combinations...\n",
      "Fitting 3 folds for each of 324 candidates, totalling 972 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best F1 Score: 0.9510\n",
      "Best Parameters:\n",
      "  C: 0.001\n",
      "  max_iter: 1000\n",
      "  penalty: None\n",
      "  solver: saga\n",
      "  tol: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def tune_logistic_regression(df, selected_features, label_column='label', n_splits=5, random_state=42, n_jobs=-1, verbose=1):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning for logistic regression focusing on iterations, \n",
    "    regularization strength, convergence tolerance, and penalty.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The dataset to use for tuning.\n",
    "        selected_features (list): List of selected features to use for model training.\n",
    "        label_column (str): Name of the column containing the target labels.\n",
    "        n_splits (int): Number of cross-validation splits.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "        n_jobs (int): Number of jobs to run in parallel (-1 means using all processors).\n",
    "        verbose (int): Verbosity level (0: no output, 1: progress bar, >1: detailed output).\n",
    "        \n",
    "    Returns:\n",
    "        best_params (dict): Dictionary containing the best hyperparameters.\n",
    "        best_score (float): The best cross-validated F1 score.\n",
    "        cv_results (dict): Full results from the grid search cross-validation.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import make_scorer, f1_score\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract features and target\n",
    "    X = df[selected_features].values\n",
    "    y = df[label_column].values\n",
    "    \n",
    "    # Define the parameter grid to search\n",
    "    param_grid = {\n",
    "        'max_iter': [1000, 2000, 5000],\n",
    "        'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        'tol': [1e-5, 1e-4, 1e-3],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', None]\n",
    "    }\n",
    "    \n",
    "    # Create a custom parameter grid that respects solver/penalty compatibility\n",
    "    # We'll use 'saga' solver which supports all penalty types\n",
    "    compatible_params = []\n",
    "    \n",
    "    for max_iter in param_grid['max_iter']:\n",
    "        for C in param_grid['C']:\n",
    "            for tol in param_grid['tol']:\n",
    "                for penalty in param_grid['penalty']:\n",
    "                    if penalty == 'elasticnet':\n",
    "                        for l1_ratio in [0.2, 0.5, 0.8]:  # Add some l1_ratio values for elasticnet\n",
    "                            compatible_params.append({\n",
    "                                'max_iter': [max_iter],\n",
    "                                'C': [C],\n",
    "                                'tol': [tol],\n",
    "                                'penalty': [penalty],\n",
    "                                'solver': ['saga'],\n",
    "                                'l1_ratio': [l1_ratio]\n",
    "                            })\n",
    "                    else:\n",
    "                        compatible_params.append({\n",
    "                            'max_iter': [max_iter],\n",
    "                            'C': [C],\n",
    "                            'tol': [tol],\n",
    "                            'penalty': [penalty],\n",
    "                            'solver': ['saga']\n",
    "                        })\n",
    "    \n",
    "    # Define the logistic regression model\n",
    "    lr = LogisticRegression(random_state=random_state, n_jobs=1)\n",
    "    \n",
    "    # Define the scoring metric (F1 score)\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=lr,\n",
    "        param_grid=compatible_params,\n",
    "        scoring=f1_scorer,\n",
    "        cv=n_splits,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=verbose,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting grid search with {len(compatible_params)} parameter combinations...\")\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # Get the best parameters and score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    print(f\"\\nBest F1 Score: {best_score:.4f}\")\n",
    "    print(\"Best Parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Return the best parameters, best score, and full results\n",
    "    return best_params, best_score, grid_search.cv_results_\n",
    "\n",
    "print(\"tuning with best features: \", best_features)\n",
    "tuning_set = train_data.sample(frac=0.1, random_state=42)\n",
    "best_params, best_score, cv_results = tune_logistic_regression(tuning_set, best_features, label_column='label', n_splits=3, random_state=42, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9325 (Â±0.0012)\n",
      "Precision: 0.9122 (Â±0.0016)\n",
      "Recall: 0.9968 (Â±0.0002)\n",
      "F1: 0.9526 (Â±0.0009)\n",
      "True positive rate: 0.9968 (Â±0.0002)\n",
      "False positive rate: 0.2045 (Â±0.0036)\n",
      "True negative rate: 0.7955 (Â±0.0036)\n",
      "False negative rate: 0.0032 (Â±0.0002)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# do k fold cross validation with the best features and best params\n",
    "results = perform_logistic_regression_cv(train_data, best_features, label_column='label', n_splits=5, **best_params)\n",
    "pretty_print_results(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1192: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\joe\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(train_df, test_df, selected_features, label_column, best_params):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression model on the entire training set using the optimal feature subset and hyperparameters,\n",
    "    and then evaluates the model performance on the testing set.\n",
    "\n",
    "    Parameters:\n",
    "      train_df (pd.DataFrame): The training dataset.\n",
    "      test_df (pd.DataFrame): The testing dataset.\n",
    "      selected_features (list): List of optimal features to use for training.\n",
    "      label_column (str): The name of the target label column.\n",
    "      best_params (dict): A dictionary of optimal hyperparameters for logistic regression \n",
    "                          (e.g., {'max_iter': 5000, 'C': 1.0, 'tol': 1e-4, 'penalty': 'l2', 'solver': 'saga'}).\n",
    "\n",
    "    Returns:\n",
    "      results (dict): A dictionary containing evaluation metrics:\n",
    "                      - accuracy: Accuracy score on the test set.\n",
    "                      - precision: Precision score.\n",
    "                      - recall: Recall score.\n",
    "                      - f1: F1 score.\n",
    "                      - true_positive_rate: Fraction of positive samples correctly classified.\n",
    "                      - true_negative_rate: Fraction of negative samples correctly classified.\n",
    "                      - false_positive_rate: Fraction of negative samples incorrectly classified as positive.\n",
    "                      - false_negative_rate: Fraction of positive samples incorrectly classified as negative.\n",
    "      model (LogisticRegression): The trained logistic regression model.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "    # Prepare training features and labels.\n",
    "    X_train = train_df[selected_features].values\n",
    "    y_train = train_df[label_column].values\n",
    "\n",
    "    # Prepare testing features and labels.\n",
    "    X_test = test_df[selected_features].values\n",
    "    y_test = test_df[label_column].values\n",
    "\n",
    "    # Initialize the logistic regression model with optimal parameters.\n",
    "    # Ensure random_state is set for reproducibility.\n",
    "    model = LogisticRegression(random_state=42, **best_params)\n",
    "    \n",
    "    # Train the model on the entire training set.\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the testing set.\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute the confusion matrix and derive rates.\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    false_negative_rate = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    true_positive_rate = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    true_negative_rate = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # Calculate evaluation metrics.\n",
    "    results = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'true_positive_rate': true_positive_rate,\n",
    "        'true_negative_rate': true_negative_rate,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'false_negative_rate': false_negative_rate,\n",
    "    }\n",
    "    \n",
    "    return results, model\n",
    "\n",
    "results, trained_model = train_and_evaluate_model(\n",
    "    train_df=train_data, \n",
    "    test_df=test_data, \n",
    "    selected_features=best_features, \n",
    "    label_column='label', \n",
    "    best_params=best_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8094301122285381\n",
      "precision: 0.7436302067922543\n",
      "recall: 0.9979264096002823\n",
      "f1: 0.8522125727634082\n",
      "true_positive_rate: 0.9979264096002823\n",
      "true_negative_rate: 0.5784864864864865\n",
      "false_positive_rate: 0.4215135135135135\n",
      "false_negative_rate: 0.002073590399717639\n"
     ]
    }
   ],
   "source": [
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook i will explore the effectiveness of using logistic regression on the UNSW_NB15 intrusion detection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will preprocess the data by performing the log transformations and then encoding categorical features as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log transformation applied to numeric features (if present) in the dataset\n",
      "Log transformation applied to numeric features (if present) in the dataset\n",
      "One-hot encoding applied to categorical features in the dataset\n",
      "Updated categorical feature columns: ['proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_any', 'proto_argus', 'proto_aris', 'proto_arp', 'proto_ax.25', 'proto_bbn-rcc', 'proto_bna', 'proto_br-sat-mon', 'proto_cbt', 'proto_cftp', 'proto_chaos', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crtp', 'proto_crudp', 'proto_dcn', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_egp', 'proto_eigrp', 'proto_emcon', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_ggp', 'proto_gmtp', 'proto_gre', 'proto_hmp', 'proto_i-nlsp', 'proto_iatp', 'proto_ib', 'proto_icmp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_igmp', 'proto_igp', 'proto_il', 'proto_ip', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ipnip', 'proto_ippc', 'proto_ipv6', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_irtp', 'proto_isis', 'proto_iso-ip', 'proto_iso-tp4', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_leaf-1', 'proto_leaf-2', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mobile', 'proto_mtp', 'proto_mux', 'proto_narp', 'proto_netblt', 'proto_nsfnet-igp', 'proto_nvp', 'proto_ospf', 'proto_pgm', 'proto_pim', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_prm', 'proto_ptp', 'proto_pup', 'proto_pvp', 'proto_qnx', 'proto_rdp', 'proto_rsvp', 'proto_rtp', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sctp', 'proto_sdrp', 'proto_secure-vmtp', 'proto_sep', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_st2', 'proto_stp', 'proto_sun-nd', 'proto_swipe', 'proto_tcf', 'proto_tcp', 'proto_tlsp', 'proto_tp++', 'proto_trunk-1', 'proto_trunk-2', 'proto_ttp', 'proto_udp', 'proto_unas', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xnet', 'proto_xns-idp', 'proto_xtp', 'proto_zero', 'state_CON', 'state_ECO', 'state_FIN', 'state_INT', 'state_PAR', 'state_REQ', 'state_RST', 'state_URN', 'state_no', 'service_-', 'service_dhcp', 'service_dns', 'service_ftp', 'service_ftp-data', 'service_http', 'service_irc', 'service_pop3', 'service_radius', 'service_smtp', 'service_snmp', 'service_ssh', 'service_ssl', 'is_sm_ips_ports_0', 'is_sm_ips_ports_1', 'is_ftp_login_0', 'is_ftp_login_1', 'is_ftp_login_2', 'is_ftp_login_4']\n",
      "One-hot encoding applied to categorical features in the dataset\n",
      "Updated categorical feature columns: ['proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_any', 'proto_argus', 'proto_aris', 'proto_arp', 'proto_ax.25', 'proto_bbn-rcc', 'proto_bna', 'proto_br-sat-mon', 'proto_cbt', 'proto_cftp', 'proto_chaos', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crtp', 'proto_crudp', 'proto_dcn', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_egp', 'proto_eigrp', 'proto_emcon', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_ggp', 'proto_gmtp', 'proto_gre', 'proto_hmp', 'proto_i-nlsp', 'proto_iatp', 'proto_ib', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_igmp', 'proto_igp', 'proto_il', 'proto_ip', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ipnip', 'proto_ippc', 'proto_ipv6', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_irtp', 'proto_isis', 'proto_iso-ip', 'proto_iso-tp4', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_leaf-1', 'proto_leaf-2', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mobile', 'proto_mtp', 'proto_mux', 'proto_narp', 'proto_netblt', 'proto_nsfnet-igp', 'proto_nvp', 'proto_ospf', 'proto_pgm', 'proto_pim', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_prm', 'proto_ptp', 'proto_pup', 'proto_pvp', 'proto_qnx', 'proto_rdp', 'proto_rsvp', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sctp', 'proto_sdrp', 'proto_secure-vmtp', 'proto_sep', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_st2', 'proto_stp', 'proto_sun-nd', 'proto_swipe', 'proto_tcf', 'proto_tcp', 'proto_tlsp', 'proto_tp++', 'proto_trunk-1', 'proto_trunk-2', 'proto_ttp', 'proto_udp', 'proto_unas', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xnet', 'proto_xns-idp', 'proto_xtp', 'proto_zero', 'state_ACC', 'state_CLO', 'state_CON', 'state_FIN', 'state_INT', 'state_REQ', 'state_RST', 'service_-', 'service_dhcp', 'service_dns', 'service_ftp', 'service_ftp-data', 'service_http', 'service_irc', 'service_pop3', 'service_radius', 'service_smtp', 'service_snmp', 'service_ssh', 'service_ssl', 'is_sm_ips_ports_0', 'is_sm_ips_ports_1', 'is_ftp_login_0', 'is_ftp_login_1', 'is_ftp_login_2']\n",
      "Training data shape: (175341, 199)\n",
      "Testing data shape: (82332, 194)\n",
      "Any non-numeric columns remaining in Training data: False\n",
      "List of columns in the Training Data:\n",
      "['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'label', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_any', 'proto_argus', 'proto_aris', 'proto_arp', 'proto_ax.25', 'proto_bbn-rcc', 'proto_bna', 'proto_br-sat-mon', 'proto_cbt', 'proto_cftp', 'proto_chaos', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crtp', 'proto_crudp', 'proto_dcn', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_egp', 'proto_eigrp', 'proto_emcon', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_ggp', 'proto_gmtp', 'proto_gre', 'proto_hmp', 'proto_i-nlsp', 'proto_iatp', 'proto_ib', 'proto_icmp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_igmp', 'proto_igp', 'proto_il', 'proto_ip', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ipnip', 'proto_ippc', 'proto_ipv6', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_irtp', 'proto_isis', 'proto_iso-ip', 'proto_iso-tp4', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_leaf-1', 'proto_leaf-2', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mobile', 'proto_mtp', 'proto_mux', 'proto_narp', 'proto_netblt', 'proto_nsfnet-igp', 'proto_nvp', 'proto_ospf', 'proto_pgm', 'proto_pim', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_prm', 'proto_ptp', 'proto_pup', 'proto_pvp', 'proto_qnx', 'proto_rdp', 'proto_rsvp', 'proto_rtp', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sctp', 'proto_sdrp', 'proto_secure-vmtp', 'proto_sep', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_st2', 'proto_stp', 'proto_sun-nd', 'proto_swipe', 'proto_tcf', 'proto_tcp', 'proto_tlsp', 'proto_tp++', 'proto_trunk-1', 'proto_trunk-2', 'proto_ttp', 'proto_udp', 'proto_unas', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xnet', 'proto_xns-idp', 'proto_xtp', 'proto_zero', 'state_CON', 'state_ECO', 'state_FIN', 'state_INT', 'state_PAR', 'state_REQ', 'state_RST', 'state_URN', 'state_no', 'service_-', 'service_dhcp', 'service_dns', 'service_ftp', 'service_ftp-data', 'service_http', 'service_irc', 'service_pop3', 'service_radius', 'service_smtp', 'service_snmp', 'service_ssh', 'service_ssl', 'is_sm_ips_ports_0', 'is_sm_ips_ports_1', 'is_ftp_login_0', 'is_ftp_login_1', 'is_ftp_login_2', 'is_ftp_login_4']\n",
      "\n",
      "List of columns in the Testing Data:\n",
      "['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'label', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_any', 'proto_argus', 'proto_aris', 'proto_arp', 'proto_ax.25', 'proto_bbn-rcc', 'proto_bna', 'proto_br-sat-mon', 'proto_cbt', 'proto_cftp', 'proto_chaos', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crtp', 'proto_crudp', 'proto_dcn', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_egp', 'proto_eigrp', 'proto_emcon', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_ggp', 'proto_gmtp', 'proto_gre', 'proto_hmp', 'proto_i-nlsp', 'proto_iatp', 'proto_ib', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_igmp', 'proto_igp', 'proto_il', 'proto_ip', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ipnip', 'proto_ippc', 'proto_ipv6', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_irtp', 'proto_isis', 'proto_iso-ip', 'proto_iso-tp4', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_leaf-1', 'proto_leaf-2', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mobile', 'proto_mtp', 'proto_mux', 'proto_narp', 'proto_netblt', 'proto_nsfnet-igp', 'proto_nvp', 'proto_ospf', 'proto_pgm', 'proto_pim', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_prm', 'proto_ptp', 'proto_pup', 'proto_pvp', 'proto_qnx', 'proto_rdp', 'proto_rsvp', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sctp', 'proto_sdrp', 'proto_secure-vmtp', 'proto_sep', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_st2', 'proto_stp', 'proto_sun-nd', 'proto_swipe', 'proto_tcf', 'proto_tcp', 'proto_tlsp', 'proto_tp++', 'proto_trunk-1', 'proto_trunk-2', 'proto_ttp', 'proto_udp', 'proto_unas', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xnet', 'proto_xns-idp', 'proto_xtp', 'proto_zero', 'state_ACC', 'state_CLO', 'state_CON', 'state_FIN', 'state_INT', 'state_REQ', 'state_RST', 'service_-', 'service_dhcp', 'service_dns', 'service_ftp', 'service_ftp-data', 'service_http', 'service_irc', 'service_pop3', 'service_radius', 'service_smtp', 'service_snmp', 'service_ssh', 'service_ssl', 'is_sm_ips_ports_0', 'is_sm_ips_ports_1', 'is_ftp_login_0', 'is_ftp_login_1', 'is_ftp_login_2']\n"
     ]
    }
   ],
   "source": [
    "from typing import List  # Import type hints for better code clarity\n",
    "\n",
    "# Define lists of features for preprocessing:\n",
    "# - 'categorical_features' will be one-hot encoded.\n",
    "# - 'features_to_transform' will undergo a log transformation to reduce skewness.\n",
    "categorical_features: List[str] = [\"proto\", \"state\", \"service\", \"is_sm_ips_ports\", \"is_ftp_login\"]\n",
    "features_to_transform: List[str] = [\n",
    "    'sbytes', 'dbytes', 'sttl', 'dttl', 'sload', 'dload', 'spkts', 'dpkts', \n",
    "    'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'sjit', 'djit'\n",
    "]\n",
    "\n",
    "# Load the training and testing datasets from CSV files.\n",
    "train_data: pd.DataFrame = pd.read_csv('../data/UNSW_NB15/UNSW_NB15_training-set.csv')\n",
    "test_data: pd.DataFrame = pd.read_csv('../data/UNSW_NB15/UNSW_NB15_testing-set.csv')\n",
    "\n",
    "# Clean both datasets by removing columns that are not needed for modeling.\n",
    "columns_to_drop = ['attack_cat', 'id']\n",
    "for df in [train_data, test_data]:\n",
    "    for col in columns_to_drop:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "def process_numeric_features(df: pd.DataFrame, features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies a natural logarithm transformation (ln(x+1)) to each specified numeric feature \n",
    "    in the provided dataframe. This helps to normalize the distribution of features and \n",
    "    mitigate the effect of extreme values.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        if feature in df.columns:\n",
    "            df[feature] = np.log1p(df[feature])\n",
    "    print(\"Log transformation applied to numeric features (if present) in the dataset\")\n",
    "    return df\n",
    "\n",
    "def process_categorical_features(df: pd.DataFrame, cat_features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One-hot encodes specified categorical features in the provided dataframe.\n",
    "    Processes each feature independently. If a feature is missing, a warning is printed.\n",
    "    Returns:\n",
    "        df: Updated dataframe with one-hot encoded categorical features.\n",
    "        updated_dummy_cols: List of the names of all the new categorical features.\n",
    "    \"\"\"\n",
    "    for feature in cat_features:\n",
    "        if feature in df.columns:\n",
    "            dummies = pd.get_dummies(df[feature].astype(str), prefix=feature)\n",
    "            df = df.drop(columns=[feature])\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "        else:\n",
    "            print(f\"Warning: '{feature}' not found in the dataframe; skipping one-hot encoding for this feature.\")\n",
    "    print(\"One-hot encoding applied to categorical features in the dataset\")\n",
    "    updated_dummy_cols = [col for col in df.columns if any(col.startswith(f\"{feature}_\") for feature in cat_features)]\n",
    "    print(\"Updated categorical feature columns:\", updated_dummy_cols)\n",
    "    return df, updated_dummy_cols\n",
    "\n",
    "# Process numeric features on each dataset independently.\n",
    "train_data = process_numeric_features(train_data, features_to_transform)\n",
    "test_data = process_numeric_features(test_data, features_to_transform)\n",
    "\n",
    "# Process categorical features on each dataset independently.\n",
    "train_data, train_categorical_features = process_categorical_features(train_data, categorical_features)\n",
    "test_data, test_categorical_features = process_categorical_features(test_data, categorical_features)\n",
    "\n",
    "# Calculate the union of the new categorical feature sets from training and testing datasets.\n",
    "# (The previous code was calculating the intersection.)\n",
    "categorical_features = [i for i in train_categorical_features if i in test_categorical_features]\n",
    "\n",
    "# Output the shapes of the processed datasets and confirm that all features are numeric.\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "print(f\"Any non-numeric columns remaining in Training data: {any(not pd.api.types.is_numeric_dtype(train_data[col]) for col in train_data.columns)}\")\n",
    "print(\"List of columns in the Training Data:\")\n",
    "print(list(train_data.columns))\n",
    "print(\"\\nList of columns in the Testing Data:\")\n",
    "print(list(test_data.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function that will do k-fold cross validation and train our model. this willbe used throughout the remainder of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_logistic_regression_cv(df, feature_columns, label_column='label', n_splits=5, random_state=42):\n",
    "    # This function performs k-fold cross-validation for a logistic regression model.\n",
    "    # The model uses L2 regularization with the saga solver for optimization.\n",
    "    \n",
    "    # Import necessary libraries for model training, evaluation, and progress monitoring.\n",
    "    from sklearn.linear_model import LogisticRegression  # For building the logistic regression model\n",
    "    from sklearn.model_selection import KFold            # For splitting data into folds for cross-validation\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix  # To compute performance metrics\n",
    "    import numpy as np                                     # For numerical computations (mean, std, etc.)\n",
    "    from tqdm.notebook import tqdm                          # To display a progress bar during cross-validation\n",
    "    \n",
    "    # Convert the provided dataframe columns into numpy arrays for efficient computation.\n",
    "    X = df[feature_columns].values  # Feature matrix constructed from specified columns\n",
    "    y = df[label_column].values       # Target vector extracted from the label column\n",
    "    \n",
    "    # Set up K-Fold cross-validation with shuffling for randomness.\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Initialize lists to store metrics for each fold.\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "    tpr_list, fpr_list, tnr_list, fnr_list = [], [], [], []\n",
    "    \n",
    "    # Iterate over each train-test split generated by KFold.\n",
    "    for train_index, test_index in tqdm(kf.split(X), total=n_splits, desc=\"Cross-validation\"):\n",
    "        # Split the dataset into training and test sets for the current fold.\n",
    "        X_train, X_test = X[train_index], X[test_index]  # Extract training and testing features.\n",
    "        y_train, y_test = y[train_index], y[test_index]     # Extract corresponding target labels.\n",
    "        \n",
    "        # Initialize the Logistic Regression model with specified hyperparameters.\n",
    "        model = LogisticRegression(\n",
    "            penalty='l2',              # Use L2 regularization to mitigate overfitting.\n",
    "            C=1.0,                     # Inverse of regularization strength; a default value of 1.0.\n",
    "            solver='saga',             # Saga solver supports L2 penalty and works well with large datasets.\n",
    "            max_iter=5000,             # Maximum number of iterations to ensure convergence.\n",
    "            tol=1e-4,                  # Convergence tolerance; training stops when improvements fall below this threshold.\n",
    "            random_state=random_state, # Set random state for reproducibility.\n",
    "            n_jobs=-1,                 # Utilize all available CPU cores for computation.\n",
    "            verbose=False               # Enable verbose output to monitor model training progress.\n",
    "        )\n",
    "        \n",
    "        # Fit the model on the training data.\n",
    "        model.fit(X_train, y_train)\n",
    "        # Use the trained model to predict target labels on the test set.\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Compute performance metrics for the current fold and append them to the lists.\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))  # Overall accuracy of predictions.\n",
    "        precisions.append(precision_score(y_test, y_pred, zero_division=0))  # Precision; handles division by zero.\n",
    "        recalls.append(recall_score(y_test, y_pred, zero_division=0))  # Recall; handles division by zero.\n",
    "        f1_scores.append(f1_score(y_test, y_pred, zero_division=0))    # F1 score; harmonic mean of precision and recall.\n",
    "        \n",
    "        # Generate confusion matrix and unpack into true negatives, false positives, false negatives, and true positives.\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        # Calculate and store the True Positive Rate (Sensitivity) for the fold.\n",
    "        tpr_list.append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        # Calculate and store the False Positive Rate for the fold.\n",
    "        fpr_list.append(fp / (fp + tn) if (fp + tn) > 0 else 0)\n",
    "        # Calculate and store the True Negative Rate (Specificity) for the fold.\n",
    "        tnr_list.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "        # Calculate and store the False Negative Rate for the fold.\n",
    "        fnr_list.append(fn / (fn + tp) if (fn + tp) > 0 else 0)\n",
    "    \n",
    "    # Compile the computed metrics into a dictionary by calculating the mean and standard deviation across folds.\n",
    "    results = {\n",
    "        'accuracy': {'mean': np.mean(accuracies), 'std': np.std(accuracies)},\n",
    "        'precision': {'mean': np.mean(precisions), 'std': np.std(precisions)},\n",
    "        'recall': {'mean': np.mean(recalls), 'std': np.std(recalls)},\n",
    "        'f1': {'mean': np.mean(f1_scores), 'std': np.std(f1_scores)},\n",
    "        'true_positive_rate': {'mean': np.mean(tpr_list), 'std': np.std(tpr_list)},\n",
    "        'false_positive_rate': {'mean': np.mean(fpr_list), 'std': np.std(fpr_list)},\n",
    "        'true_negative_rate': {'mean': np.mean(tnr_list), 'std': np.std(tnr_list)},\n",
    "        'false_negative_rate': {'mean': np.mean(fnr_list), 'std': np.std(fnr_list)}\n",
    "    }\n",
    "    # Return the aggregated cross-validation results.\n",
    "    return results\n",
    "\n",
    "def pretty_print_results(results):\n",
    "    # Iterate through each metric in the results dictionary.\n",
    "    for metric, values in results.items():\n",
    "        # Format and print each metric's name, mean, and standard deviation.\n",
    "        print(f\"{metric.replace('_', ' ').capitalize()}: {values['mean']:.4f} (±{values['std']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will do some experimentation on sets of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 percentile features:\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate']\n",
      "\n",
      "Top 40 percentile features:\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate', 'swin', 'sload', 'dwin', 'stcpb', 'dtcpb', 'ct_src_dport_ltm', 'ct_dst_src_ltm', 'sbytes', 'dttl']\n",
      "\n",
      "Top 60 percentile features:\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate', 'swin', 'sload', 'dwin', 'stcpb', 'dtcpb', 'ct_src_dport_ltm', 'ct_dst_src_ltm', 'sbytes', 'dttl', 'ct_src_ltm', 'ct_dst_ltm', 'ct_srv_src', 'ct_srv_dst', 'sinpkt', 'djit', 'sjit', 'ackdat', 'dloss']\n",
      "\n",
      "Top 80 percentile features:\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate', 'swin', 'sload', 'dwin', 'stcpb', 'dtcpb', 'ct_src_dport_ltm', 'ct_dst_src_ltm', 'sbytes', 'dttl', 'ct_src_ltm', 'ct_dst_ltm', 'ct_srv_src', 'ct_srv_dst', 'sinpkt', 'djit', 'sjit', 'ackdat', 'dloss', 'tcprtt', 'synack', 'dur', 'dinpkt', 'response_body_len', 'ct_flw_http_mthd', 'proto_icmp', 'state_ECO', 'ct_ftp_cmd']\n",
      "\n",
      "All correlated features (in sorted order):\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate', 'swin', 'sload', 'dwin', 'stcpb', 'dtcpb', 'ct_src_dport_ltm', 'ct_dst_src_ltm', 'sbytes', 'dttl', 'ct_src_ltm', 'ct_dst_ltm', 'ct_srv_src', 'ct_srv_dst', 'sinpkt', 'djit', 'sjit', 'ackdat', 'dloss', 'tcprtt', 'synack', 'dur', 'dinpkt', 'response_body_len', 'ct_flw_http_mthd', 'proto_icmp', 'state_ECO', 'ct_ftp_cmd', 'trans_depth', 'smean', 'is_ftp_login_4', 'proto_rtp', 'state_URN', 'state_no', 'state_PAR', 'sloss']\n"
     ]
    }
   ],
   "source": [
    "# Detailed Numeric Feature Selection Process\n",
    "# ------------------------------------------------------------\n",
    "# This section identifies and ranks numeric features based on their absolute Pearson correlation\n",
    "# with the target label. We exclude both the 'label' column and any categorical features.\n",
    "#\n",
    "# The process involves:\n",
    "# 1. Extracting numeric features from the training dataset.\n",
    "# 2. Computing the absolute correlation of each feature with the target variable.\n",
    "# 3. Sorting features in descending order based on their correlation.\n",
    "# 4. Creating feature subsets corresponding to the top 20%, 40%, 60%, and 80% of features,\n",
    "#    in addition to a full sorted list of all features.\n",
    "#\n",
    "# These feature subsets can help in selecting the most impactful variables for model training.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Step 1: Extract numeric features by excluding 'label' and categorical features.\n",
    "numeric_features = [col for col in train_data.columns if col != 'label' and col not in categorical_features]\n",
    "\n",
    "# Safety check: Remove 'label' if it is inadvertently included.\n",
    "if 'label' in numeric_features:\n",
    "    numeric_features.remove('label')\n",
    "\n",
    "# Step 2: Compute the absolute Pearson correlation between each numeric feature and the target label.\n",
    "correlations = []\n",
    "for feature in numeric_features:\n",
    "    try:\n",
    "        corr_value = abs(train_data[feature].corr(train_data['label']))\n",
    "        correlations.append((feature, corr_value))\n",
    "    except KeyError:\n",
    "        print(f\"Error: Unable to calculate correlation for feature '{feature}'.\")\n",
    "\n",
    "# Step 3: Sort the features by their correlation strength in descending order.\n",
    "sorted_correlations = sorted(correlations, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Step 4: Determine indices for the top percentiles.\n",
    "n_features = len(sorted_correlations)\n",
    "index_20 = int(np.ceil(n_features * 0.20))\n",
    "index_40 = int(np.ceil(n_features * 0.40))\n",
    "index_60 = int(np.ceil(n_features * 0.60))\n",
    "index_80 = int(np.ceil(n_features * 0.80))\n",
    "\n",
    "# Create lists of features for each specified percentile.\n",
    "top_20_numeric_features = [feature for feature, _ in sorted_correlations[:index_20]]\n",
    "top_40_numeric_features = [feature for feature, _ in sorted_correlations[:index_40]]\n",
    "top_60_numeric_features = [feature for feature, _ in sorted_correlations[:index_60]]\n",
    "top_80_numeric_features = [feature for feature, _ in sorted_correlations[:index_80]]\n",
    "all_correlated_features = [feature for feature, _ in sorted_correlations]\n",
    "\n",
    "# Display the feature groups.\n",
    "print(\"\\nTop 20 percentile features:\")\n",
    "print(top_20_numeric_features)\n",
    "\n",
    "print(\"\\nTop 40 percentile features:\")\n",
    "print(top_40_numeric_features)\n",
    "\n",
    "print(\"\\nTop 60 percentile features:\")\n",
    "print(top_60_numeric_features)\n",
    "\n",
    "print(\"\\nTop 80 percentile features:\")\n",
    "print(top_80_numeric_features)\n",
    "\n",
    "print(\"\\nAll correlated features (in sorted order):\")\n",
    "print(all_correlated_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 percentile categorical features:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n']\n",
      "\n",
      "Top 40 percentile categorical features:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_aris', 'proto_ax.25', 'proto_bna', 'proto_br-sat-mon', 'proto_cftp', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crudp', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_eigrp', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_gmtp', 'proto_i-nlsp', 'proto_iatp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_il', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ippc']\n",
      "\n",
      "Top 60 percentile categorical features:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_aris', 'proto_ax.25', 'proto_bna', 'proto_br-sat-mon', 'proto_cftp', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crudp', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_eigrp', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_gmtp', 'proto_i-nlsp', 'proto_iatp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_il', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ippc', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_isis', 'proto_iso-ip', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mtp', 'proto_narp', 'proto_nsfnet-igp', 'proto_pgm', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_ptp', 'proto_pvp', 'proto_qnx', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sdrp', 'proto_secure-vmtp']\n",
      "\n",
      "Top 80 percentile categorical features:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_aris', 'proto_ax.25', 'proto_bna', 'proto_br-sat-mon', 'proto_cftp', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crudp', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_eigrp', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_gmtp', 'proto_i-nlsp', 'proto_iatp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_il', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ippc', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_isis', 'proto_iso-ip', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mtp', 'proto_narp', 'proto_nsfnet-igp', 'proto_pgm', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_ptp', 'proto_pvp', 'proto_qnx', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sdrp', 'proto_secure-vmtp', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_stp', 'proto_tcf', 'proto_tlsp', 'proto_tp++', 'proto_ttp', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xtp', 'proto_zero', 'proto_cbt', 'proto_chaos', 'proto_crtp', 'proto_dcn', 'proto_emcon', 'proto_ggp', 'proto_igp', 'proto_ip', 'proto_ipnip']\n",
      "\n",
      "All categorical features sorted by Chi-squared score:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_aris', 'proto_ax.25', 'proto_bna', 'proto_br-sat-mon', 'proto_cftp', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crudp', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_eigrp', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_gmtp', 'proto_i-nlsp', 'proto_iatp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_il', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ippc', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_isis', 'proto_iso-ip', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mtp', 'proto_narp', 'proto_nsfnet-igp', 'proto_pgm', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_ptp', 'proto_pvp', 'proto_qnx', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sdrp', 'proto_secure-vmtp', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_stp', 'proto_tcf', 'proto_tlsp', 'proto_tp++', 'proto_ttp', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xtp', 'proto_zero', 'proto_cbt', 'proto_chaos', 'proto_crtp', 'proto_dcn', 'proto_emcon', 'proto_ggp', 'proto_igp', 'proto_ip', 'proto_ipnip', 'proto_irtp', 'proto_iso-tp4', 'proto_leaf-1', 'proto_leaf-2', 'proto_mux', 'proto_nvp', 'proto_prm', 'proto_pup', 'proto_st2', 'proto_trunk-1', 'proto_trunk-2', 'proto_xnet', 'proto_xns-idp', 'proto_argus', 'proto_bbn-rcc', 'proto_egp', 'proto_hmp', 'proto_netblt', 'proto_rdp', 'service_dhcp', 'proto_igmp', 'service_snmp', 'is_ftp_login_1', 'service_ssl', 'service_ftp', 'service_irc', 'service_radius', 'service_smtp', 'is_ftp_login_0', 'is_ftp_login_2']\n"
     ]
    }
   ],
   "source": [
    "# Since the categorical features have already been one-hot encoded, we can use them directly.\n",
    "onehot_cat_features = train_data[categorical_features]\n",
    "\n",
    "# Step 2: Compute Chi-squared scores for each one-hot encoded categorical feature using the target label.\n",
    "from sklearn.feature_selection import chi2\n",
    "chi2_scores, p_values = chi2(onehot_cat_features, train_data['label'])\n",
    "\n",
    "# Step 3: Pair each categorical feature with its Chi-squared score and sort in descending order.\n",
    "cat_chi2_scores = list(zip(categorical_features, chi2_scores))\n",
    "sorted_cat_chi2 = sorted(cat_chi2_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 4: Determine indices corresponding to the top percentiles of categorical features.\n",
    "n_cat_features = len(sorted_cat_chi2)\n",
    "index_20_cat = int(np.ceil(n_cat_features * 0.20))\n",
    "index_40_cat = int(np.ceil(n_cat_features * 0.40))\n",
    "index_60_cat = int(np.ceil(n_cat_features * 0.60))\n",
    "index_80_cat = int(np.ceil(n_cat_features * 0.80))\n",
    "\n",
    "# Step 5: Create lists of categorical features for each specified percentile.\n",
    "top_20_cat_features = [feature for feature, _ in sorted_cat_chi2[:index_20_cat]]\n",
    "top_40_cat_features = [feature for feature, _ in sorted_cat_chi2[:index_40_cat]]\n",
    "top_60_cat_features = [feature for feature, _ in sorted_cat_chi2[:index_60_cat]]\n",
    "top_80_cat_features = [feature for feature, _ in sorted_cat_chi2[:index_80_cat]]\n",
    "all_chi2_cat_features = [feature for feature, _ in sorted_cat_chi2]\n",
    "\n",
    "# Step 6: Display the ranked categorical feature groups.\n",
    "print(\"\\nTop 20 percentile categorical features:\")\n",
    "print(top_20_cat_features)\n",
    "\n",
    "print(\"\\nTop 40 percentile categorical features:\")\n",
    "print(top_40_cat_features)\n",
    "\n",
    "print(\"\\nTop 60 percentile categorical features:\")\n",
    "print(top_60_cat_features)\n",
    "\n",
    "print(\"\\nTop 80 percentile categorical features:\")\n",
    "print(top_80_cat_features)\n",
    "\n",
    "print(\"\\nAll categorical features sorted by Chi-squared score:\")\n",
    "print(all_chi2_cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run logistic regression cross-validation on various selections of categorical features\n",
    "# obtained from the Chi-squared ranking. Each block evaluates performance when using a different\n",
    "# top percentile of features, and finally a baseline using all the original categorical features.\n",
    "\n",
    "# Evaluate with the top 20 percentile of categorical features.\n",
    "print(\"\\nPerforming logistic regression with top 20 percentile categorical features only:\")\n",
    "results_top_20 = perform_logistic_regression_cv(train_data, top_20_cat_features)\n",
    "pretty_print_results(results_top_20)\n",
    "\n",
    "# # Evaluate with the top 40 percentile of categorical features.\n",
    "# print(\"\\nPerforming logistic regression with top 40 percentile categorical features only:\")\n",
    "# results_top_40 = perform_logistic_regression_cv(train_data, top_40_cat_features)\n",
    "# pretty_print_results(results_top_40)\n",
    "\n",
    "# # Evaluate with the top 60 percentile of categorical features.\n",
    "# print(\"\\nPerforming logistic regression with top 60 percentile categorical features only:\")\n",
    "# results_top_60 = perform_logistic_regression_cv(train_data, top_60_cat_features)\n",
    "# pretty_print_results(results_top_60)\n",
    "\n",
    "# # Evaluate with the top 80 percentile of categorical features.\n",
    "# print(\"\\nPerforming logistic regression with top 80 percentile categorical features only:\")\n",
    "# results_top_80 = perform_logistic_regression_cv(train_data, top_80_cat_features)\n",
    "# pretty_print_results(results_top_80)\n",
    "\n",
    "# # Evaluate with all categorical features as a baseline.\n",
    "# print(\"\\nPerforming logistic regression with all categorical features:\")\n",
    "# results_all = perform_logistic_regression_cv(train_data, categorical_features)\n",
    "# pretty_print_results(results_all)\n",
    "\n",
    "# running all the results will yield that there is really no difference in performance between the top 20 and other percentile features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try numeric features with different percentiles\n",
    "print(\"\\nPerforming logistic regression with top 20 percentile numeric features only:\")\n",
    "results_numeric_top_20 = perform_logistic_regression_cv(train_data, top_20_numeric_features)\n",
    "pretty_print_results(results_numeric_top_20)\n",
    "\n",
    "# print(\"\\nPerforming logistic regression with top 40 percentile numeric features only:\")\n",
    "# results_numeric_top_40 = perform_logistic_regression_cv(train_data, top_40_numeric_features)\n",
    "# pretty_print_results(results_numeric_top_40)\n",
    "\n",
    "# print(\"\\nPerforming logistic regression with top 60 percentile numeric features only:\")\n",
    "# results_numeric_top_60 = perform_logistic_regression_cv(train_data, top_60_numeric_features)\n",
    "# pretty_print_results(results_numeric_top_60)\n",
    "\n",
    "# print(\"\\nPerforming logistic regression with top 80 percentile numeric features only:\")\n",
    "# results_numeric_top_80 = perform_logistic_regression_cv(train_data, top_80_numeric_features)\n",
    "# pretty_print_results(results_numeric_top_80)\n",
    "\n",
    "# # do it for all numeric features\n",
    "# print(\"\\nPerforming logistic regression with all numeric features:\")\n",
    "# results_numeric_all = perform_logistic_regression_cv(train_data, numeric_features)\n",
    "# pretty_print_results(results_numeric_all)\n",
    "\n",
    "# there is a similar deiminishing return here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 1 features: ['service_http' 'sload' 'service_http' 'state_REQ' 'is_sm_ips_ports_0'\n",
      " 'is_sm_ips_ports_1' 'proto_mobile' 'sbytes' 'service_http' 'swin']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a198fc01b061440e928a7ce7e0c80b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cross-validation:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7366 (±0.0062)\n",
      "Precision: 0.8005 (±0.0022)\n",
      "Recall: 0.8165 (±0.0114)\n",
      "F1: 0.8084 (±0.0059)\n",
      "True positive rate: 0.8165 (±0.0114)\n",
      "False positive rate: 0.4335 (±0.0058)\n",
      "True negative rate: 0.5665 (±0.0058)\n",
      "False negative rate: 0.1835 (±0.0114)\n",
      "Random sample 2 features: ['service_ftp-data' 'stcpb' 'service_ssh' 'ct_src_dport_ltm' 'proto_sctp'\n",
      " 'is_sm_ips_ports_1' 'ct_state_ttl' 'spkts' 'service_-' 'service_ssh']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143cac24120a4eeba15184078b0ede30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cross-validation:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # now try numeric features with different percentiles\n",
    "# print(\"\\nPerforming logistic regression with top 20 percentile numeric and categorical features together:\")\n",
    "# results_numeric_top_20 = perform_logistic_regression_cv(train_data, top_40_numeric_features+top_20_cat_features)\n",
    "# pretty_print_results(results_numeric_top_20)\n",
    "\n",
    "combined_features = top_40_numeric_features+top_20_cat_features\n",
    "\n",
    "for i in range(4):\n",
    "    # Select a random sample of 10 features from the combined features\n",
    "    random_features = np.random.choice(combined_features, size=10, replace=True)\n",
    "    print(f\"Random sample {i+1} features: {random_features}\")\n",
    "    \n",
    "    # Perform cross validation using the selected features\n",
    "    results_random = perform_logistic_regression_cv(train_data, random_features)\n",
    "    pretty_print_results(results_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook i will explore the effectiveness of using logistic regression on the UNSW_NB15 intrusion detection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will preprocess the data by performing the log transformations and then encoding categorical features as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log transformation applied to numeric features (if present) in the dataset\n",
      "Log transformation applied to numeric features (if present) in the dataset\n",
      "One-hot encoding applied to categorical features in the dataset\n",
      "Updated categorical feature columns: ['proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_any', 'proto_argus', 'proto_aris', 'proto_arp', 'proto_ax.25', 'proto_bbn-rcc', 'proto_bna', 'proto_br-sat-mon', 'proto_cbt', 'proto_cftp', 'proto_chaos', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crtp', 'proto_crudp', 'proto_dcn', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_egp', 'proto_eigrp', 'proto_emcon', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_ggp', 'proto_gmtp', 'proto_gre', 'proto_hmp', 'proto_i-nlsp', 'proto_iatp', 'proto_ib', 'proto_icmp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_igmp', 'proto_igp', 'proto_il', 'proto_ip', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ipnip', 'proto_ippc', 'proto_ipv6', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_irtp', 'proto_isis', 'proto_iso-ip', 'proto_iso-tp4', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_leaf-1', 'proto_leaf-2', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mobile', 'proto_mtp', 'proto_mux', 'proto_narp', 'proto_netblt', 'proto_nsfnet-igp', 'proto_nvp', 'proto_ospf', 'proto_pgm', 'proto_pim', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_prm', 'proto_ptp', 'proto_pup', 'proto_pvp', 'proto_qnx', 'proto_rdp', 'proto_rsvp', 'proto_rtp', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sctp', 'proto_sdrp', 'proto_secure-vmtp', 'proto_sep', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_st2', 'proto_stp', 'proto_sun-nd', 'proto_swipe', 'proto_tcf', 'proto_tcp', 'proto_tlsp', 'proto_tp++', 'proto_trunk-1', 'proto_trunk-2', 'proto_ttp', 'proto_udp', 'proto_unas', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xnet', 'proto_xns-idp', 'proto_xtp', 'proto_zero', 'state_CON', 'state_ECO', 'state_FIN', 'state_INT', 'state_PAR', 'state_REQ', 'state_RST', 'state_URN', 'state_no', 'service_-', 'service_dhcp', 'service_dns', 'service_ftp', 'service_ftp-data', 'service_http', 'service_irc', 'service_pop3', 'service_radius', 'service_smtp', 'service_snmp', 'service_ssh', 'service_ssl', 'is_sm_ips_ports_0', 'is_sm_ips_ports_1', 'is_ftp_login_0', 'is_ftp_login_1', 'is_ftp_login_2', 'is_ftp_login_4']\n",
      "One-hot encoding applied to categorical features in the dataset\n",
      "Updated categorical feature columns: ['proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_any', 'proto_argus', 'proto_aris', 'proto_arp', 'proto_ax.25', 'proto_bbn-rcc', 'proto_bna', 'proto_br-sat-mon', 'proto_cbt', 'proto_cftp', 'proto_chaos', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crtp', 'proto_crudp', 'proto_dcn', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_egp', 'proto_eigrp', 'proto_emcon', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_ggp', 'proto_gmtp', 'proto_gre', 'proto_hmp', 'proto_i-nlsp', 'proto_iatp', 'proto_ib', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_igmp', 'proto_igp', 'proto_il', 'proto_ip', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ipnip', 'proto_ippc', 'proto_ipv6', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_irtp', 'proto_isis', 'proto_iso-ip', 'proto_iso-tp4', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_leaf-1', 'proto_leaf-2', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mobile', 'proto_mtp', 'proto_mux', 'proto_narp', 'proto_netblt', 'proto_nsfnet-igp', 'proto_nvp', 'proto_ospf', 'proto_pgm', 'proto_pim', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_prm', 'proto_ptp', 'proto_pup', 'proto_pvp', 'proto_qnx', 'proto_rdp', 'proto_rsvp', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sctp', 'proto_sdrp', 'proto_secure-vmtp', 'proto_sep', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_st2', 'proto_stp', 'proto_sun-nd', 'proto_swipe', 'proto_tcf', 'proto_tcp', 'proto_tlsp', 'proto_tp++', 'proto_trunk-1', 'proto_trunk-2', 'proto_ttp', 'proto_udp', 'proto_unas', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xnet', 'proto_xns-idp', 'proto_xtp', 'proto_zero', 'state_ACC', 'state_CLO', 'state_CON', 'state_FIN', 'state_INT', 'state_REQ', 'state_RST', 'service_-', 'service_dhcp', 'service_dns', 'service_ftp', 'service_ftp-data', 'service_http', 'service_irc', 'service_pop3', 'service_radius', 'service_smtp', 'service_snmp', 'service_ssh', 'service_ssl', 'is_sm_ips_ports_0', 'is_sm_ips_ports_1', 'is_ftp_login_0', 'is_ftp_login_1', 'is_ftp_login_2']\n",
      "Training data shape: (175341, 199)\n",
      "Testing data shape: (82332, 194)\n",
      "Any non-numeric columns remaining in Training data: False\n",
      "List of columns in the Training Data:\n",
      "['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'label', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_any', 'proto_argus', 'proto_aris', 'proto_arp', 'proto_ax.25', 'proto_bbn-rcc', 'proto_bna', 'proto_br-sat-mon', 'proto_cbt', 'proto_cftp', 'proto_chaos', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crtp', 'proto_crudp', 'proto_dcn', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_egp', 'proto_eigrp', 'proto_emcon', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_ggp', 'proto_gmtp', 'proto_gre', 'proto_hmp', 'proto_i-nlsp', 'proto_iatp', 'proto_ib', 'proto_icmp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_igmp', 'proto_igp', 'proto_il', 'proto_ip', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ipnip', 'proto_ippc', 'proto_ipv6', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_irtp', 'proto_isis', 'proto_iso-ip', 'proto_iso-tp4', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_leaf-1', 'proto_leaf-2', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mobile', 'proto_mtp', 'proto_mux', 'proto_narp', 'proto_netblt', 'proto_nsfnet-igp', 'proto_nvp', 'proto_ospf', 'proto_pgm', 'proto_pim', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_prm', 'proto_ptp', 'proto_pup', 'proto_pvp', 'proto_qnx', 'proto_rdp', 'proto_rsvp', 'proto_rtp', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sctp', 'proto_sdrp', 'proto_secure-vmtp', 'proto_sep', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_st2', 'proto_stp', 'proto_sun-nd', 'proto_swipe', 'proto_tcf', 'proto_tcp', 'proto_tlsp', 'proto_tp++', 'proto_trunk-1', 'proto_trunk-2', 'proto_ttp', 'proto_udp', 'proto_unas', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xnet', 'proto_xns-idp', 'proto_xtp', 'proto_zero', 'state_CON', 'state_ECO', 'state_FIN', 'state_INT', 'state_PAR', 'state_REQ', 'state_RST', 'state_URN', 'state_no', 'service_-', 'service_dhcp', 'service_dns', 'service_ftp', 'service_ftp-data', 'service_http', 'service_irc', 'service_pop3', 'service_radius', 'service_smtp', 'service_snmp', 'service_ssh', 'service_ssl', 'is_sm_ips_ports_0', 'is_sm_ips_ports_1', 'is_ftp_login_0', 'is_ftp_login_1', 'is_ftp_login_2', 'is_ftp_login_4']\n",
      "\n",
      "List of columns in the Testing Data:\n",
      "['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'label', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_any', 'proto_argus', 'proto_aris', 'proto_arp', 'proto_ax.25', 'proto_bbn-rcc', 'proto_bna', 'proto_br-sat-mon', 'proto_cbt', 'proto_cftp', 'proto_chaos', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crtp', 'proto_crudp', 'proto_dcn', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_egp', 'proto_eigrp', 'proto_emcon', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_ggp', 'proto_gmtp', 'proto_gre', 'proto_hmp', 'proto_i-nlsp', 'proto_iatp', 'proto_ib', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_igmp', 'proto_igp', 'proto_il', 'proto_ip', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ipnip', 'proto_ippc', 'proto_ipv6', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_irtp', 'proto_isis', 'proto_iso-ip', 'proto_iso-tp4', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_leaf-1', 'proto_leaf-2', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mobile', 'proto_mtp', 'proto_mux', 'proto_narp', 'proto_netblt', 'proto_nsfnet-igp', 'proto_nvp', 'proto_ospf', 'proto_pgm', 'proto_pim', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_prm', 'proto_ptp', 'proto_pup', 'proto_pvp', 'proto_qnx', 'proto_rdp', 'proto_rsvp', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sctp', 'proto_sdrp', 'proto_secure-vmtp', 'proto_sep', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_st2', 'proto_stp', 'proto_sun-nd', 'proto_swipe', 'proto_tcf', 'proto_tcp', 'proto_tlsp', 'proto_tp++', 'proto_trunk-1', 'proto_trunk-2', 'proto_ttp', 'proto_udp', 'proto_unas', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xnet', 'proto_xns-idp', 'proto_xtp', 'proto_zero', 'state_ACC', 'state_CLO', 'state_CON', 'state_FIN', 'state_INT', 'state_REQ', 'state_RST', 'service_-', 'service_dhcp', 'service_dns', 'service_ftp', 'service_ftp-data', 'service_http', 'service_irc', 'service_pop3', 'service_radius', 'service_smtp', 'service_snmp', 'service_ssh', 'service_ssl', 'is_sm_ips_ports_0', 'is_sm_ips_ports_1', 'is_ftp_login_0', 'is_ftp_login_1', 'is_ftp_login_2']\n"
     ]
    }
   ],
   "source": [
    "from typing import List  # Import type hints for better code clarity\n",
    "\n",
    "# Define lists of features for preprocessing:\n",
    "# - 'categorical_features' will be one-hot encoded.\n",
    "# - 'features_to_transform' will undergo a log transformation to reduce skewness.\n",
    "categorical_features: List[str] = [\"proto\", \"state\", \"service\", \"is_sm_ips_ports\", \"is_ftp_login\"]\n",
    "features_to_transform: List[str] = [\n",
    "    'sbytes', 'dbytes', 'sttl', 'dttl', 'sload', 'dload', 'spkts', 'dpkts', \n",
    "    'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'sjit', 'djit'\n",
    "]\n",
    "\n",
    "# Load the training and testing datasets from CSV files.\n",
    "train_data: pd.DataFrame = pd.read_csv('../data/UNSW_NB15/UNSW_NB15_training-set.csv')\n",
    "test_data: pd.DataFrame = pd.read_csv('../data/UNSW_NB15/UNSW_NB15_testing-set.csv')\n",
    "\n",
    "# Clean both datasets by removing columns that are not needed for modeling.\n",
    "columns_to_drop = ['attack_cat', 'id']\n",
    "for df in [train_data, test_data]:\n",
    "    for col in columns_to_drop:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "def process_numeric_features(df: pd.DataFrame, features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies a natural logarithm transformation (ln(x+1)) to each specified numeric feature \n",
    "    in the provided dataframe. This helps to normalize the distribution of features and \n",
    "    mitigate the effect of extreme values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # apply the log transformation to the features that were determined in EDA\n",
    "    for feature in features:\n",
    "        if feature in df.columns:\n",
    "            df[feature] = np.log1p(df[feature])\n",
    "    print(\"Log transformation applied to numeric features (if present) in the dataset\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_categorical_features(df: pd.DataFrame, cat_features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One-hot encodes specified categorical features in the provided dataframe.\n",
    "    Processes each feature independently. If a feature is missing, a warning is printed.\n",
    "    Returns:\n",
    "        df: Updated dataframe with one-hot encoded categorical features.\n",
    "        updated_dummy_cols: List of the names of all the new categorical features.\n",
    "    \"\"\"\n",
    "    for feature in cat_features:\n",
    "        if feature in df.columns:\n",
    "            dummies = pd.get_dummies(df[feature].astype(str), prefix=feature)\n",
    "            df = df.drop(columns=[feature])\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "        else:\n",
    "            print(f\"Warning: '{feature}' not found in the dataframe; skipping one-hot encoding for this feature.\")\n",
    "    print(\"One-hot encoding applied to categorical features in the dataset\")\n",
    "    updated_dummy_cols = [col for col in df.columns if any(col.startswith(f\"{feature}_\") for feature in cat_features)]\n",
    "    print(\"Updated categorical feature columns:\", updated_dummy_cols)\n",
    "    return df, updated_dummy_cols\n",
    "\n",
    "# Process numeric features on each dataset independently.\n",
    "train_data = process_numeric_features(train_data, features_to_transform)\n",
    "test_data = process_numeric_features(test_data, features_to_transform)\n",
    "\n",
    "# Process categorical features on each dataset independently.\n",
    "train_data, train_categorical_features = process_categorical_features(train_data, categorical_features)\n",
    "test_data, test_categorical_features = process_categorical_features(test_data, categorical_features)\n",
    "\n",
    "# Calculate the union of the new categorical feature sets from training and testing datasets.\n",
    "# (The previous code was calculating the intersection.)\n",
    "categorical_features = [i for i in train_categorical_features if i in test_categorical_features]\n",
    "\n",
    "# Output the shapes of the processed datasets and confirm that all features are numeric.\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "print(f\"Any non-numeric columns remaining in Training data: {any(not pd.api.types.is_numeric_dtype(train_data[col]) for col in train_data.columns)}\")\n",
    "print(\"List of columns in the Training Data:\")\n",
    "print(list(train_data.columns))\n",
    "print(\"\\nList of columns in the Testing Data:\")\n",
    "print(list(test_data.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(df, label_column='label'):\n",
    "    \"\"\"\n",
    "    Computes balanced class weights for a given dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the label column\n",
    "        label_column: Name of the column containing class labels (default: 'label')\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping class indices to their weights\n",
    "    \"\"\"\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    \n",
    "    # Get class distribution\n",
    "    class_counts = df[label_column].value_counts()\n",
    "    print(\"Class distribution:\", class_counts)\n",
    "    \n",
    "    # Compute balanced weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(df[label_column]),\n",
    "        y=df[label_column]\n",
    "    )\n",
    "    \n",
    "    # Convert to dictionary mapping class indices to weights\n",
    "    return {i: weight for i, weight in enumerate(class_weights)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function that will do k-fold cross validation and train our model. this willbe used throughout the remainder of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_logistic_regression_cv(df, feature_columns, label_column='label', n_splits=5, random_state=42,\n",
    "                                   penalty='l2', C=1.0, solver='saga', max_iter=5000, tol=1e-4, n_jobs=-1, verbose=False,\n",
    "                                   class_weight=None):\n",
    "    # This function performs k-fold cross-validation for a logistic regression model.\n",
    "    # The model uses the hyperparameters provided as additional parameters. The defaults are set as:\n",
    "    #   penalty: 'l2'\n",
    "    #   C: 1.0\n",
    "    #   solver: 'saga'\n",
    "    #   max_iter: 5000\n",
    "    #   tol: 1e-4\n",
    "    #   n_jobs: -1\n",
    "    #   verbose: False\n",
    "    #   class_weight: None\n",
    "    \n",
    "    # Import necessary libraries for model training and evaluation.\n",
    "    from sklearn.linear_model import LogisticRegression  # For building the logistic regression model\n",
    "    from sklearn.model_selection import KFold            # For splitting data into folds for cross-validation\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix  # To compute performance metrics\n",
    "    import numpy as np                                     # For numerical computations (mean, std, etc.)\n",
    "    \n",
    "    # Convert the provided dataframe columns into numpy arrays for efficient computation.\n",
    "    X = df[feature_columns].values  # Feature matrix constructed from specified columns\n",
    "    y = df[label_column].values       # Target vector extracted from the label column\n",
    "    \n",
    "    # Set up K-Fold cross-validation with shuffling for randomness.\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Initialize lists to store metrics for each fold.\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "    tpr_list, fpr_list, tnr_list, fnr_list = [], [], [], []\n",
    "    \n",
    "    # Iterate over each train-test split generated by KFold.\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the dataset into training and test sets for the current fold.\n",
    "        X_train, X_test = X[train_index], X[test_index]  # Extract training and testing features.\n",
    "        y_train, y_test = y[train_index], y[test_index]    # Extract corresponding target labels.\n",
    "        \n",
    "        # Initialize the Logistic Regression model with provided hyperparameters.\n",
    "        model = LogisticRegression(\n",
    "            penalty=penalty,              # Regularization method.\n",
    "            C=C,                          # Inverse of regularization strength.\n",
    "            solver=solver,                # Algorithm to use in the optimization problem.\n",
    "            max_iter=max_iter,            # Maximum number of iterations to ensure convergence.\n",
    "            tol=tol,                      # Tolerance for convergence.\n",
    "            random_state=random_state,    # Set random state for reproducibility.\n",
    "            n_jobs=n_jobs,                # Utilize the specified number of CPU cores.\n",
    "            verbose=verbose,              # Verbose output mode.\n",
    "            class_weight=class_weight     # Class weights for imbalanced datasets.\n",
    "        )\n",
    "        \n",
    "        # Fit the model on the training data.\n",
    "        model.fit(X_train, y_train)\n",
    "        # Use the trained model to predict target labels on the test set.\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Compute performance metrics for the current fold and append them to the lists.\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))               # Overall accuracy of predictions.\n",
    "        precisions.append(precision_score(y_test, y_pred, zero_division=0))  # Precision; handles division by zero.\n",
    "        recalls.append(recall_score(y_test, y_pred, zero_division=0))         # Recall; handles division by zero.\n",
    "        f1_scores.append(f1_score(y_test, y_pred, zero_division=0))           # F1 score; harmonic mean of precision and recall.\n",
    "        \n",
    "        # Generate confusion matrix and unpack into true negatives, false positives, false negatives, and true positives.\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        # Calculate and store the True Positive Rate (Sensitivity) for the fold.\n",
    "        tpr_list.append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        # Calculate and store the False Positive Rate for the fold.\n",
    "        fpr_list.append(fp / (fp + tn) if (fp + tn) > 0 else 0)\n",
    "        # Calculate and store the True Negative Rate (Specificity) for the fold.\n",
    "        tnr_list.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "        # Calculate and store the False Negative Rate for the fold.\n",
    "        fnr_list.append(fn / (fn + tp) if (fn + tp) > 0 else 0)\n",
    "    \n",
    "    # Compile the computed metrics into a dictionary by calculating the mean and standard deviation across folds.\n",
    "    results = {\n",
    "        'accuracy': {'mean': np.mean(accuracies), 'std': np.std(accuracies)},\n",
    "        'precision': {'mean': np.mean(precisions), 'std': np.std(precisions)},\n",
    "        'recall': {'mean': np.mean(recalls), 'std': np.std(recalls)},\n",
    "        'f1': {'mean': np.mean(f1_scores), 'std': np.std(f1_scores)},\n",
    "        'true_positive_rate': {'mean': np.mean(tpr_list), 'std': np.std(tpr_list)},\n",
    "        'false_positive_rate': {'mean': np.mean(fpr_list), 'std': np.std(fpr_list)},\n",
    "        'true_negative_rate': {'mean': np.mean(tnr_list), 'std': np.std(tnr_list)},\n",
    "        'false_negative_rate': {'mean': np.mean(fnr_list), 'std': np.std(fnr_list)}\n",
    "    }\n",
    "    # Return the aggregated cross-validation results.\n",
    "    return results\n",
    "\n",
    "def pretty_print_results(results):\n",
    "    # Iterate through each metric in the results dictionary.\n",
    "    for metric, values in results.items():\n",
    "        # Format and print each metric's name, mean, and standard deviation.\n",
    "        print(f\"{metric.replace('_', ' ').capitalize()}: {values['mean']:.4f} (±{values['std']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will do some experimentation on sets of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 percentile features:\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate']\n",
      "\n",
      "Top 40 percentile features:\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate', 'swin', 'sload', 'dwin', 'stcpb', 'dtcpb', 'ct_src_dport_ltm', 'ct_dst_src_ltm', 'sbytes', 'dttl']\n",
      "\n",
      "Top 60 percentile features:\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate', 'swin', 'sload', 'dwin', 'stcpb', 'dtcpb', 'ct_src_dport_ltm', 'ct_dst_src_ltm', 'sbytes', 'dttl', 'ct_src_ltm', 'ct_dst_ltm', 'ct_srv_src', 'ct_srv_dst', 'sinpkt', 'djit', 'sjit', 'ackdat', 'dloss']\n",
      "\n",
      "Top 80 percentile features:\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate', 'swin', 'sload', 'dwin', 'stcpb', 'dtcpb', 'ct_src_dport_ltm', 'ct_dst_src_ltm', 'sbytes', 'dttl', 'ct_src_ltm', 'ct_dst_ltm', 'ct_srv_src', 'ct_srv_dst', 'sinpkt', 'djit', 'sjit', 'ackdat', 'dloss', 'tcprtt', 'synack', 'dur', 'dinpkt', 'response_body_len', 'ct_flw_http_mthd', 'proto_icmp', 'state_ECO', 'ct_ftp_cmd']\n",
      "\n",
      "All correlated features (in sorted order):\n",
      "['sttl', 'dload', 'ct_state_ttl', 'dbytes', 'dpkts', 'ct_dst_sport_ltm', 'spkts', 'dmean', 'rate', 'swin', 'sload', 'dwin', 'stcpb', 'dtcpb', 'ct_src_dport_ltm', 'ct_dst_src_ltm', 'sbytes', 'dttl', 'ct_src_ltm', 'ct_dst_ltm', 'ct_srv_src', 'ct_srv_dst', 'sinpkt', 'djit', 'sjit', 'ackdat', 'dloss', 'tcprtt', 'synack', 'dur', 'dinpkt', 'response_body_len', 'ct_flw_http_mthd', 'proto_icmp', 'state_ECO', 'ct_ftp_cmd', 'trans_depth', 'smean', 'is_ftp_login_4', 'proto_rtp', 'state_URN', 'state_no', 'state_PAR', 'sloss']\n"
     ]
    }
   ],
   "source": [
    "# Detailed Numeric Feature Selection Process\n",
    "# ------------------------------------------------------------\n",
    "# This section identifies and ranks numeric features based on their absolute Pearson correlation\n",
    "# with the target label. We exclude both the 'label' column and any categorical features.\n",
    "#\n",
    "# The process involves:\n",
    "# 1. Extracting numeric features from the training dataset.\n",
    "# 2. Computing the absolute correlation of each feature with the target variable.\n",
    "# 3. Sorting features in descending order based on their correlation.\n",
    "# 4. Creating feature subsets corresponding to the top 20%, 40%, 60%, and 80% of features,\n",
    "#    in addition to a full sorted list of all features.\n",
    "#\n",
    "# These feature subsets can help in selecting the most impactful variables for model training.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Step 1: Extract numeric features by excluding 'label' and categorical features.\n",
    "numeric_features = [col for col in train_data.columns if col != 'label' and col not in categorical_features]\n",
    "\n",
    "# Safety check: Remove 'label' if it is inadvertently included.\n",
    "if 'label' in numeric_features:\n",
    "    numeric_features.remove('label')\n",
    "\n",
    "# Step 2: Compute the absolute Pearson correlation between each numeric feature and the target label.\n",
    "correlations = []\n",
    "for feature in numeric_features:\n",
    "    try:\n",
    "        corr_value = abs(train_data[feature].corr(train_data['label']))\n",
    "        correlations.append((feature, corr_value))\n",
    "    except KeyError:\n",
    "        print(f\"Error: Unable to calculate correlation for feature '{feature}'.\")\n",
    "\n",
    "# Step 3: Sort the features by their correlation strength in descending order.\n",
    "sorted_correlations = sorted(correlations, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Step 4: Determine indices for the top percentiles.\n",
    "n_features = len(sorted_correlations)\n",
    "index_20 = int(np.ceil(n_features * 0.20))\n",
    "index_40 = int(np.ceil(n_features * 0.40))\n",
    "index_60 = int(np.ceil(n_features * 0.60))\n",
    "index_80 = int(np.ceil(n_features * 0.80))\n",
    "\n",
    "# Create lists of features for each specified percentile.\n",
    "top_20_numeric_features = [feature for feature, _ in sorted_correlations[:index_20]]\n",
    "top_40_numeric_features = [feature for feature, _ in sorted_correlations[:index_40]]\n",
    "top_60_numeric_features = [feature for feature, _ in sorted_correlations[:index_60]]\n",
    "top_80_numeric_features = [feature for feature, _ in sorted_correlations[:index_80]]\n",
    "all_correlated_features = [feature for feature, _ in sorted_correlations]\n",
    "\n",
    "# Display the feature groups.\n",
    "print(\"\\nTop 20 percentile features:\")\n",
    "print(top_20_numeric_features)\n",
    "\n",
    "print(\"\\nTop 40 percentile features:\")\n",
    "print(top_40_numeric_features)\n",
    "\n",
    "print(\"\\nTop 60 percentile features:\")\n",
    "print(top_60_numeric_features)\n",
    "\n",
    "print(\"\\nTop 80 percentile features:\")\n",
    "print(top_80_numeric_features)\n",
    "\n",
    "print(\"\\nAll correlated features (in sorted order):\")\n",
    "print(all_correlated_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 percentile categorical features:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n']\n",
      "\n",
      "Top 40 percentile categorical features:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_aris', 'proto_ax.25', 'proto_bna', 'proto_br-sat-mon', 'proto_cftp', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crudp', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_eigrp', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_gmtp', 'proto_i-nlsp', 'proto_iatp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_il', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ippc']\n",
      "\n",
      "Top 60 percentile categorical features:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_aris', 'proto_ax.25', 'proto_bna', 'proto_br-sat-mon', 'proto_cftp', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crudp', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_eigrp', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_gmtp', 'proto_i-nlsp', 'proto_iatp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_il', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ippc', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_isis', 'proto_iso-ip', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mtp', 'proto_narp', 'proto_nsfnet-igp', 'proto_pgm', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_ptp', 'proto_pvp', 'proto_qnx', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sdrp', 'proto_secure-vmtp']\n",
      "\n",
      "Top 80 percentile categorical features:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_aris', 'proto_ax.25', 'proto_bna', 'proto_br-sat-mon', 'proto_cftp', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crudp', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_eigrp', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_gmtp', 'proto_i-nlsp', 'proto_iatp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_il', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ippc', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_isis', 'proto_iso-ip', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mtp', 'proto_narp', 'proto_nsfnet-igp', 'proto_pgm', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_ptp', 'proto_pvp', 'proto_qnx', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sdrp', 'proto_secure-vmtp', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_stp', 'proto_tcf', 'proto_tlsp', 'proto_tp++', 'proto_ttp', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xtp', 'proto_zero', 'proto_cbt', 'proto_chaos', 'proto_crtp', 'proto_dcn', 'proto_emcon', 'proto_ggp', 'proto_igp', 'proto_ip', 'proto_ipnip']\n",
      "\n",
      "All categorical features sorted by Chi-squared score:\n",
      "['state_INT', 'state_CON', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'proto_unas', 'service_dns', 'proto_udp', 'service_ssh', 'service_-', 'service_ftp-data', 'proto_ospf', 'proto_sctp', 'service_pop3', 'state_REQ', 'proto_any', 'state_RST', 'proto_gre', 'service_http', 'proto_ipv6', 'proto_mobile', 'proto_pim', 'proto_sun-nd', 'proto_swipe', 'is_sm_ips_ports_0', 'proto_rsvp', 'proto_sep', 'proto_ib', 'proto_3pc', 'proto_a/n', 'proto_aes-sp3-d', 'proto_aris', 'proto_ax.25', 'proto_bna', 'proto_br-sat-mon', 'proto_cftp', 'proto_compaq-peer', 'proto_cphb', 'proto_cpnx', 'proto_crudp', 'proto_ddp', 'proto_ddx', 'proto_dgp', 'proto_eigrp', 'proto_encap', 'proto_etherip', 'proto_fc', 'proto_fire', 'proto_gmtp', 'proto_i-nlsp', 'proto_iatp', 'proto_idpr', 'proto_idpr-cmtp', 'proto_idrp', 'proto_ifmp', 'proto_il', 'proto_ipcomp', 'proto_ipcv', 'proto_ipip', 'proto_iplt', 'proto_ippc', 'proto_ipv6-frag', 'proto_ipv6-no', 'proto_ipv6-opts', 'proto_ipv6-route', 'proto_ipx-n-ip', 'proto_isis', 'proto_iso-ip', 'proto_kryptolan', 'proto_l2tp', 'proto_larp', 'proto_merit-inp', 'proto_mfe-nsp', 'proto_mhrp', 'proto_micp', 'proto_mtp', 'proto_narp', 'proto_nsfnet-igp', 'proto_pgm', 'proto_pipe', 'proto_pnni', 'proto_pri-enc', 'proto_ptp', 'proto_pvp', 'proto_qnx', 'proto_rvd', 'proto_sat-expak', 'proto_sat-mon', 'proto_sccopmce', 'proto_scps', 'proto_sdrp', 'proto_secure-vmtp', 'proto_skip', 'proto_sm', 'proto_smp', 'proto_snp', 'proto_sprite-rpc', 'proto_sps', 'proto_srp', 'proto_stp', 'proto_tcf', 'proto_tlsp', 'proto_tp++', 'proto_ttp', 'proto_uti', 'proto_vines', 'proto_visa', 'proto_vmtp', 'proto_vrrp', 'proto_wb-expak', 'proto_wb-mon', 'proto_wsn', 'proto_xtp', 'proto_zero', 'proto_cbt', 'proto_chaos', 'proto_crtp', 'proto_dcn', 'proto_emcon', 'proto_ggp', 'proto_igp', 'proto_ip', 'proto_ipnip', 'proto_irtp', 'proto_iso-tp4', 'proto_leaf-1', 'proto_leaf-2', 'proto_mux', 'proto_nvp', 'proto_prm', 'proto_pup', 'proto_st2', 'proto_trunk-1', 'proto_trunk-2', 'proto_xnet', 'proto_xns-idp', 'proto_argus', 'proto_bbn-rcc', 'proto_egp', 'proto_hmp', 'proto_netblt', 'proto_rdp', 'service_dhcp', 'proto_igmp', 'service_snmp', 'is_ftp_login_1', 'service_ssl', 'service_ftp', 'service_irc', 'service_radius', 'service_smtp', 'is_ftp_login_0', 'is_ftp_login_2']\n"
     ]
    }
   ],
   "source": [
    "# Since the categorical features have already been one-hot encoded, we can use them directly.\n",
    "onehot_cat_features = train_data[categorical_features]\n",
    "\n",
    "# Step 2: Compute Chi-squared scores for each one-hot encoded categorical feature using the target label.\n",
    "from sklearn.feature_selection import chi2\n",
    "chi2_scores, p_values = chi2(onehot_cat_features, train_data['label'])\n",
    "\n",
    "# Step 3: Pair each categorical feature with its Chi-squared score and sort in descending order.\n",
    "cat_chi2_scores = list(zip(categorical_features, chi2_scores))\n",
    "sorted_cat_chi2 = sorted(cat_chi2_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 4: Determine indices corresponding to the top percentiles of categorical features.\n",
    "n_cat_features = len(sorted_cat_chi2)\n",
    "index_20_cat = int(np.ceil(n_cat_features * 0.20))\n",
    "index_40_cat = int(np.ceil(n_cat_features * 0.40))\n",
    "index_60_cat = int(np.ceil(n_cat_features * 0.60))\n",
    "index_80_cat = int(np.ceil(n_cat_features * 0.80))\n",
    "\n",
    "# Step 5: Create lists of categorical features for each specified percentile.\n",
    "top_20_cat_features = [feature for feature, _ in sorted_cat_chi2[:index_20_cat]]\n",
    "top_40_cat_features = [feature for feature, _ in sorted_cat_chi2[:index_40_cat]]\n",
    "top_60_cat_features = [feature for feature, _ in sorted_cat_chi2[:index_60_cat]]\n",
    "top_80_cat_features = [feature for feature, _ in sorted_cat_chi2[:index_80_cat]]\n",
    "all_chi2_cat_features = [feature for feature, _ in sorted_cat_chi2]\n",
    "\n",
    "# Step 6: Display the ranked categorical feature groups.\n",
    "print(\"\\nTop 20 percentile categorical features:\")\n",
    "print(top_20_cat_features)\n",
    "\n",
    "print(\"\\nTop 40 percentile categorical features:\")\n",
    "print(top_40_cat_features)\n",
    "\n",
    "print(\"\\nTop 60 percentile categorical features:\")\n",
    "print(top_60_cat_features)\n",
    "\n",
    "print(\"\\nTop 80 percentile categorical features:\")\n",
    "print(top_80_cat_features)\n",
    "\n",
    "print(\"\\nAll categorical features sorted by Chi-squared score:\")\n",
    "print(all_chi2_cat_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we present a hill climbing function that we will use to select different sets of features based on what we have calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_rfe(df, feature_columns, label_column='label', n_features_to_select=10, \n",
    "                step=1, estimator=None, cv=5, scoring='f1', random_state=42, verbose=1, class_weight=None):\n",
    "    \"\"\"\n",
    "    Performs Recursive Feature Elimination (RFE) with cross-validation to select the optimal features.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The dataset containing features and target variable.\n",
    "        feature_columns (list): List of feature column names to consider for selection.\n",
    "        label_column (str): Name of the target variable column.\n",
    "        n_features_to_select (int or float): Number of features to select. If float between 0 and 1,\n",
    "                                            it represents the proportion of features to select.\n",
    "        step (int or float): Number of features to remove at each iteration. If float between 0 and 1,\n",
    "                             it represents the proportion of features to remove at each iteration.\n",
    "        estimator (object): A supervised learning estimator with a fit method. If None, uses LogisticRegression.\n",
    "        cv (int): Number of cross-validation folds.\n",
    "        scoring (str): Scoring metric to use for feature selection.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "        verbose (int): Controls verbosity of output.\n",
    "        class_weight (dict or 'balanced'): Weights associated with classes. If None, all classes have weight 1.\n",
    "        \n",
    "    Returns:\n",
    "        selected_features (list): List of selected feature names.\n",
    "        rfe_cv (RFECV object): The fitted RFECV object for further inspection.\n",
    "        cv_results (dict): Cross-validation results.\n",
    "    \"\"\"\n",
    "    from sklearn.feature_selection import RFECV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Extract features and target\n",
    "    X = df[feature_columns]\n",
    "    y = df[label_column]\n",
    "    \n",
    "    # Set default estimator if none provided\n",
    "    if estimator is None:\n",
    "        estimator = LogisticRegression(max_iter=5000, random_state=random_state, class_weight=class_weight)\n",
    "    \n",
    "    # Initialize RFECV\n",
    "    rfe_cv = RFECV(\n",
    "        estimator=estimator,\n",
    "        step=step,\n",
    "        min_features_to_select=n_features_to_select,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Fit RFECV\n",
    "    rfe_cv.fit(X, y)\n",
    "    \n",
    "    # Get selected features\n",
    "    selected_features = [feature for feature, selected in zip(feature_columns, rfe_cv.support_) if selected]\n",
    "    \n",
    "    \n",
    "    # Prepare CV results\n",
    "    cv_results = {\n",
    "        'n_features': rfe_cv.n_features_,\n",
    "        'cv_results_': rfe_cv.cv_results_,  # Using cv_results_ instead of grid_scores_ which is deprecated\n",
    "        'ranking': rfe_cv.ranking_,\n",
    "        'support': rfe_cv.support_\n",
    "    }\n",
    "    \n",
    "    return selected_features, rfe_cv, cv_results\n",
    "\n",
    "def pretty_print_rfecv_results(results):\n",
    "    \"\"\"\n",
    "    Pretty prints the results of the RFECV feature selection process.\n",
    "    \n",
    "    Parameters:\n",
    "        results (tuple): Tuple containing (selected_features, rfe_cv, cv_results)\n",
    "    \"\"\"\n",
    "    selected_features, rfe_cv, cv_results = results\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" \"*20 + \"FEATURE SELECTION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Print optimal number of features\n",
    "    print(f\"Optimal number of features: {cv_results['n_features']}\")\n",
    "    \n",
    "    # Print selected features\n",
    "    print(\"\\nSelected features:\")\n",
    "    for i, feature in enumerate(selected_features, 1):\n",
    "        print(f\"  {i}. {feature}\")\n",
    "    \n",
    "    # Print cross-validation scores\n",
    "    mean_scores = rfe_cv.cv_results_['mean_test_score']\n",
    "    std_scores = rfe_cv.cv_results_['std_test_score']\n",
    "    \n",
    "    print(f\"\\nBest cross-validation score: {mean_scores.max():.4f} ± {std_scores[mean_scores.argmax()]:.4f}\")\n",
    "    print(f\"Cross-validation scoring metric: {rfe_cv.scoring}\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: label\n",
      "1    35830\n",
      "0    16772\n",
      "Name: count, dtype: int64\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "\n",
      "============================================================\n",
      "                    FEATURE SELECTION RESULTS\n",
      "============================================================\n",
      "Optimal number of features: 11\n",
      "\n",
      "Selected features:\n",
      "  1. ct_state_ttl\n",
      "  2. proto_tcp\n",
      "  3. state_FIN\n",
      "  4. proto_arp\n",
      "  5. is_sm_ips_ports_1\n",
      "  6. service_dns\n",
      "  7. proto_udp\n",
      "  8. proto_sctp\n",
      "  9. service_pop3\n",
      "  10. state_REQ\n",
      "  11. is_sm_ips_ports_0\n",
      "\n",
      "Best cross-validation score: 0.9516 ± 0.0009\n",
      "Cross-validation scoring metric: f1\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hill climbing feature selection\n",
    "tuning_set = train_data.sample(frac=0.3, random_state=42)\n",
    "class_weights = get_class_weights(tuning_set)\n",
    "results = perform_rfe(tuning_set, top_20_numeric_features + top_20_cat_features, label_column='label', n_features_to_select=10, step=1, estimator=None, cv=3, scoring='f1', random_state=42, verbose=1, class_weight=class_weights)\n",
    "\n",
    "# best_features = ['proto_3pc', 'proto_sctp', 'sttl', 'proto_tcp', 'state_FIN', 'state_REQ', 'spkts', 'ct_state_ttl', 'state_CON', 'proto_sun-nd', 'proto_arp', 'dpkts']\n",
    "pretty_print_rfecv_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've selected good features, we will do hyperparameter tuning below\n",
    "```\n",
    "Best Features = ['proto_3pc', 'proto_sctp', 'sttl', 'proto_tcp', 'state_FIN', 'state_REQ', 'spkts', 'ct_state_ttl', 'state_CON', 'proto_sun-nd', 'proto_arp', 'dpkts']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning with best features:  ['ct_state_ttl', 'proto_tcp', 'state_FIN', 'proto_arp', 'is_sm_ips_ports_1', 'service_dns', 'proto_udp', 'proto_sctp', 'service_pop3', 'state_REQ', 'is_sm_ips_ports_0']\n",
      "Class distribution: label\n",
      "1    35830\n",
      "0    16772\n",
      "Name: count, dtype: int64\n",
      "Starting grid search with 60 parameter combinations...\n",
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n",
      "\n",
      "Best F1 Score: 0.9516\n",
      "Best Parameters:\n",
      "  C: 1.0\n",
      "  max_iter: 10000\n",
      "  penalty: l1\n",
      "  solver: saga\n",
      "  tol: 1e-05\n"
     ]
    }
   ],
   "source": [
    "def tune_logistic_regression(df, selected_features, label_column='label', n_splits=5, random_state=42, n_jobs=-1, verbose=1, class_weight=None):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning for logistic regression focusing on iterations, \n",
    "    regularization strength, convergence tolerance, and penalty.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The dataset to use for tuning.\n",
    "        selected_features (list): List of selected features to use for model training.\n",
    "        label_column (str): Name of the column containing the target labels.\n",
    "        n_splits (int): Number of cross-validation splits.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "        n_jobs (int): Number of jobs to run in parallel (-1 means using all processors).\n",
    "        verbose (int): Verbosity level (0: no output, 1: progress bar, >1: detailed output).\n",
    "        class_weight (dict or 'balanced', optional): Weights associated with classes. If None, all classes have weight 1.\n",
    "        \n",
    "    Returns:\n",
    "        best_params (dict): Dictionary containing the best hyperparameters.\n",
    "        best_score (float): The best cross-validated F1 score.\n",
    "        cv_results (dict): Full results from the grid search cross-validation.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import make_scorer, f1_score\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract features and target\n",
    "    X = df[selected_features].values\n",
    "    y = df[label_column].values\n",
    "    \n",
    "    # Define the parameter grid to search\n",
    "    param_grid = {\n",
    "        'max_iter': [10000],\n",
    "        'C': [0.001, 0.01, 0.1, 1.0],\n",
    "        'tol': [1e-5, 1e-6, 1e-7],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet']\n",
    "    }\n",
    "    \n",
    "    # Create a custom parameter grid that respects solver/penalty compatibility\n",
    "    # We'll use 'saga' solver which supports all penalty types\n",
    "    compatible_params = []\n",
    "    \n",
    "    for max_iter in param_grid['max_iter']:\n",
    "        for C in param_grid['C']:\n",
    "            for tol in param_grid['tol']:\n",
    "                for penalty in param_grid['penalty']:\n",
    "                    if penalty == 'elasticnet':\n",
    "                        for l1_ratio in [0.2, 0.5, 0.8]:  # Add some l1_ratio values for elasticnet\n",
    "                            compatible_params.append({\n",
    "                                'max_iter': [max_iter],\n",
    "                                'C': [C],\n",
    "                                'tol': [tol],\n",
    "                                'penalty': [penalty],\n",
    "                                'solver': ['saga'],\n",
    "                                'l1_ratio': [l1_ratio]\n",
    "                            })\n",
    "                    else:\n",
    "                        compatible_params.append({\n",
    "                            'max_iter': [max_iter],\n",
    "                            'C': [C],\n",
    "                            'tol': [tol],\n",
    "                            'penalty': [penalty],\n",
    "                            'solver': ['saga']\n",
    "                        })\n",
    "    \n",
    "    # Define the logistic regression model\n",
    "    lr = LogisticRegression(random_state=random_state, n_jobs=1, class_weight=class_weight)\n",
    "    \n",
    "    # Define the scoring metric (F1 score)\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=lr,\n",
    "        param_grid=compatible_params,\n",
    "        scoring=f1_scorer,\n",
    "        cv=n_splits,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=verbose,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting grid search with {len(compatible_params)} parameter combinations...\")\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # Get the best parameters and score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    print(f\"\\nBest F1 Score: {best_score:.4f}\")\n",
    "    print(\"Best Parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Return the best parameters, best score, and full results\n",
    "    return best_params, best_score, grid_search.cv_results_\n",
    "\n",
    "print(\"tuning with best features: \", best_features)\n",
    "tuning_set = train_data.sample(frac=0.3, random_state=42)\n",
    "class_weights = get_class_weights(tuning_set)\n",
    "best_params, best_score, cv_results = tune_logistic_regression(tuning_set, best_features, label_column='label', n_splits=3, random_state=42, n_jobs=-1, verbose=1, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: label\n",
      "1    119341\n",
      "0     56000\n",
      "Name: count, dtype: int64\n",
      "Accuracy: 0.9322 (±0.0013)\n",
      "Precision: 0.9122 (±0.0016)\n",
      "Recall: 0.9963 (±0.0003)\n",
      "F1: 0.9524 (±0.0009)\n",
      "True positive rate: 0.9963 (±0.0003)\n",
      "False positive rate: 0.2045 (±0.0036)\n",
      "True negative rate: 0.7955 (±0.0036)\n",
      "False negative rate: 0.0037 (±0.0003)\n"
     ]
    }
   ],
   "source": [
    "# do k fold cross validation with the best features and best params\n",
    "class_weights = get_class_weights(train_data)\n",
    "results = perform_logistic_regression_cv(train_data, best_features, label_column='label', n_splits=5, class_weight=class_weights, **best_params)\n",
    "pretty_print_results(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(train_df, test_df, selected_features, label_column, best_params, class_weight=None):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression model on the entire training set using the optimal feature subset and hyperparameters,\n",
    "    and then evaluates the model performance on the testing set.\n",
    "\n",
    "    Parameters:\n",
    "      train_df (pd.DataFrame): The training dataset.\n",
    "      test_df (pd.DataFrame): The testing dataset.\n",
    "      selected_features (list): List of optimal features to use for training.\n",
    "      label_column (str): The name of the target label column.\n",
    "      best_params (dict): A dictionary of optimal hyperparameters for logistic regression \n",
    "                          (e.g., {'max_iter': 5000, 'C': 1.0, 'tol': 1e-4, 'penalty': 'l2', 'solver': 'saga'}).\n",
    "      class_weight (dict or str, optional): Weights associated with classes. If not provided, all classes are \n",
    "                                           supposed to have weight one. Can be 'balanced' or a dictionary.\n",
    "\n",
    "    Returns:\n",
    "      results (dict): A dictionary containing evaluation metrics:\n",
    "                      - accuracy: Accuracy score on the test set.\n",
    "                      - precision: Precision score.\n",
    "                      - recall: Recall score.\n",
    "                      - f1: F1 score.\n",
    "                      - true_positive_rate: Fraction of positive samples correctly classified.\n",
    "                      - true_negative_rate: Fraction of negative samples correctly classified.\n",
    "                      - false_positive_rate: Fraction of negative samples incorrectly classified as positive.\n",
    "                      - false_negative_rate: Fraction of positive samples incorrectly classified as negative.\n",
    "      model (LogisticRegression): The trained logistic regression model.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "    # Prepare training features and labels.\n",
    "    X_train = train_df[selected_features].values\n",
    "    y_train = train_df[label_column].values\n",
    "\n",
    "    # Prepare testing features and labels.\n",
    "    X_test = test_df[selected_features].values\n",
    "    y_test = test_df[label_column].values\n",
    "\n",
    "    # Initialize the logistic regression model with optimal parameters.\n",
    "    # Ensure random_state is set for reproducibility.\n",
    "    model = LogisticRegression(random_state=42, class_weight=class_weight, **best_params)\n",
    "    \n",
    "    # Train the model on the entire training set.\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the testing set.\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute the confusion matrix and derive rates.\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    false_negative_rate = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    true_positive_rate = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    true_negative_rate = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # Calculate evaluation metrics.\n",
    "    results = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'true_positive_rate': true_positive_rate,\n",
    "        'true_negative_rate': true_negative_rate,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'false_negative_rate': false_negative_rate,\n",
    "    }\n",
    "    \n",
    "    return results, model\n",
    "\n",
    "results, trained_model = train_and_evaluate_model(\n",
    "    train_df=train_data, \n",
    "    test_df=test_data, \n",
    "    selected_features=best_features, \n",
    "    label_column='label', \n",
    "    best_params=best_params,\n",
    "    class_weight=class_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8092722149346548\n",
      "precision: 0.743575409809112\n",
      "recall: 0.9976396364598958\n",
      "f1: 0.8520720092696392\n",
      "true_positive_rate: 0.9976396364598958\n",
      "true_negative_rate: 0.5784864864864865\n",
      "false_positive_rate: 0.4215135135135135\n",
      "false_negative_rate: 0.0023603635401041206\n"
     ]
    }
   ],
   "source": [
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
